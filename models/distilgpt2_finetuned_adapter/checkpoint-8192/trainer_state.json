{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 8192,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00244140625,
      "grad_norm": 0.3092263638973236,
      "learning_rate": 4.996337890625e-05,
      "loss": 3.5523,
      "step": 10
    },
    {
      "epoch": 0.0048828125,
      "grad_norm": 0.7024579048156738,
      "learning_rate": 4.992268880208333e-05,
      "loss": 3.4233,
      "step": 20
    },
    {
      "epoch": 0.00732421875,
      "grad_norm": 0.6998658776283264,
      "learning_rate": 4.988199869791667e-05,
      "loss": 3.337,
      "step": 30
    },
    {
      "epoch": 0.009765625,
      "grad_norm": 0.9682797193527222,
      "learning_rate": 4.984130859375e-05,
      "loss": 3.1796,
      "step": 40
    },
    {
      "epoch": 0.01220703125,
      "grad_norm": 1.2390785217285156,
      "learning_rate": 4.980061848958333e-05,
      "loss": 3.1813,
      "step": 50
    },
    {
      "epoch": 0.0146484375,
      "grad_norm": 1.4152599573135376,
      "learning_rate": 4.975992838541667e-05,
      "loss": 3.2911,
      "step": 60
    },
    {
      "epoch": 0.01708984375,
      "grad_norm": 1.5837457180023193,
      "learning_rate": 4.971923828125e-05,
      "loss": 3.2373,
      "step": 70
    },
    {
      "epoch": 0.01953125,
      "grad_norm": 1.7336403131484985,
      "learning_rate": 4.967854817708334e-05,
      "loss": 3.1688,
      "step": 80
    },
    {
      "epoch": 0.02197265625,
      "grad_norm": 1.947265863418579,
      "learning_rate": 4.963785807291667e-05,
      "loss": 3.1384,
      "step": 90
    },
    {
      "epoch": 0.0244140625,
      "grad_norm": 1.2334431409835815,
      "learning_rate": 4.959716796875e-05,
      "loss": 3.0228,
      "step": 100
    },
    {
      "epoch": 0.02685546875,
      "grad_norm": 1.502793550491333,
      "learning_rate": 4.955647786458334e-05,
      "loss": 2.8098,
      "step": 110
    },
    {
      "epoch": 0.029296875,
      "grad_norm": 2.375678300857544,
      "learning_rate": 4.951578776041667e-05,
      "loss": 3.0133,
      "step": 120
    },
    {
      "epoch": 0.03173828125,
      "grad_norm": 1.6861224174499512,
      "learning_rate": 4.947509765625e-05,
      "loss": 2.9397,
      "step": 130
    },
    {
      "epoch": 0.0341796875,
      "grad_norm": 1.8443642854690552,
      "learning_rate": 4.943440755208334e-05,
      "loss": 3.0651,
      "step": 140
    },
    {
      "epoch": 0.03662109375,
      "grad_norm": 2.265394687652588,
      "learning_rate": 4.939371744791667e-05,
      "loss": 2.9129,
      "step": 150
    },
    {
      "epoch": 0.0390625,
      "grad_norm": 2.8421812057495117,
      "learning_rate": 4.935302734375e-05,
      "loss": 2.7887,
      "step": 160
    },
    {
      "epoch": 0.04150390625,
      "grad_norm": 2.6570937633514404,
      "learning_rate": 4.931233723958334e-05,
      "loss": 2.9111,
      "step": 170
    },
    {
      "epoch": 0.0439453125,
      "grad_norm": 2.112086772918701,
      "learning_rate": 4.927164713541667e-05,
      "loss": 2.965,
      "step": 180
    },
    {
      "epoch": 0.04638671875,
      "grad_norm": 2.6904969215393066,
      "learning_rate": 4.923095703125e-05,
      "loss": 2.8469,
      "step": 190
    },
    {
      "epoch": 0.048828125,
      "grad_norm": 2.0427138805389404,
      "learning_rate": 4.919026692708334e-05,
      "loss": 2.9622,
      "step": 200
    },
    {
      "epoch": 0.05126953125,
      "grad_norm": 3.2002525329589844,
      "learning_rate": 4.914957682291667e-05,
      "loss": 2.9715,
      "step": 210
    },
    {
      "epoch": 0.0537109375,
      "grad_norm": 2.536381244659424,
      "learning_rate": 4.9108886718750006e-05,
      "loss": 2.9135,
      "step": 220
    },
    {
      "epoch": 0.05615234375,
      "grad_norm": 2.631030321121216,
      "learning_rate": 4.906819661458334e-05,
      "loss": 2.8065,
      "step": 230
    },
    {
      "epoch": 0.05859375,
      "grad_norm": 2.1613101959228516,
      "learning_rate": 4.902750651041667e-05,
      "loss": 2.9417,
      "step": 240
    },
    {
      "epoch": 0.06103515625,
      "grad_norm": 2.753105640411377,
      "learning_rate": 4.8986816406250006e-05,
      "loss": 2.7983,
      "step": 250
    },
    {
      "epoch": 0.0634765625,
      "grad_norm": 2.1928420066833496,
      "learning_rate": 4.8946126302083337e-05,
      "loss": 2.6733,
      "step": 260
    },
    {
      "epoch": 0.06591796875,
      "grad_norm": 2.6202752590179443,
      "learning_rate": 4.890543619791667e-05,
      "loss": 2.7401,
      "step": 270
    },
    {
      "epoch": 0.068359375,
      "grad_norm": 2.9132907390594482,
      "learning_rate": 4.8864746093750005e-05,
      "loss": 2.8018,
      "step": 280
    },
    {
      "epoch": 0.07080078125,
      "grad_norm": 3.3769993782043457,
      "learning_rate": 4.8824055989583336e-05,
      "loss": 2.806,
      "step": 290
    },
    {
      "epoch": 0.0732421875,
      "grad_norm": 3.5554895401000977,
      "learning_rate": 4.878336588541667e-05,
      "loss": 2.7843,
      "step": 300
    },
    {
      "epoch": 0.07568359375,
      "grad_norm": 3.2813327312469482,
      "learning_rate": 4.8742675781250005e-05,
      "loss": 2.9331,
      "step": 310
    },
    {
      "epoch": 0.078125,
      "grad_norm": 2.6199400424957275,
      "learning_rate": 4.8701985677083336e-05,
      "loss": 2.7319,
      "step": 320
    },
    {
      "epoch": 0.08056640625,
      "grad_norm": 2.6761815547943115,
      "learning_rate": 4.8661295572916674e-05,
      "loss": 2.8128,
      "step": 330
    },
    {
      "epoch": 0.0830078125,
      "grad_norm": 2.7704696655273438,
      "learning_rate": 4.8620605468750005e-05,
      "loss": 2.709,
      "step": 340
    },
    {
      "epoch": 0.08544921875,
      "grad_norm": 3.347256898880005,
      "learning_rate": 4.8579915364583336e-05,
      "loss": 2.8459,
      "step": 350
    },
    {
      "epoch": 0.087890625,
      "grad_norm": 2.342782497406006,
      "learning_rate": 4.8539225260416674e-05,
      "loss": 2.7431,
      "step": 360
    },
    {
      "epoch": 0.09033203125,
      "grad_norm": 2.588491201400757,
      "learning_rate": 4.8498535156250005e-05,
      "loss": 2.5667,
      "step": 370
    },
    {
      "epoch": 0.0927734375,
      "grad_norm": 3.0846588611602783,
      "learning_rate": 4.8457845052083336e-05,
      "loss": 2.7864,
      "step": 380
    },
    {
      "epoch": 0.09521484375,
      "grad_norm": 3.054081678390503,
      "learning_rate": 4.8417154947916674e-05,
      "loss": 2.7475,
      "step": 390
    },
    {
      "epoch": 0.09765625,
      "grad_norm": 3.356457471847534,
      "learning_rate": 4.8376464843750005e-05,
      "loss": 2.7701,
      "step": 400
    },
    {
      "epoch": 0.10009765625,
      "grad_norm": 3.7462940216064453,
      "learning_rate": 4.8335774739583336e-05,
      "loss": 2.6634,
      "step": 410
    },
    {
      "epoch": 0.1025390625,
      "grad_norm": 5.308860778808594,
      "learning_rate": 4.8295084635416674e-05,
      "loss": 2.6005,
      "step": 420
    },
    {
      "epoch": 0.10498046875,
      "grad_norm": 4.053726673126221,
      "learning_rate": 4.8254394531250005e-05,
      "loss": 2.7118,
      "step": 430
    },
    {
      "epoch": 0.107421875,
      "grad_norm": 4.380823135375977,
      "learning_rate": 4.8213704427083336e-05,
      "loss": 2.7149,
      "step": 440
    },
    {
      "epoch": 0.10986328125,
      "grad_norm": 3.5348470211029053,
      "learning_rate": 4.8173014322916674e-05,
      "loss": 2.7053,
      "step": 450
    },
    {
      "epoch": 0.1123046875,
      "grad_norm": 3.917340040206909,
      "learning_rate": 4.8132324218750005e-05,
      "loss": 2.7196,
      "step": 460
    },
    {
      "epoch": 0.11474609375,
      "grad_norm": 3.95341157913208,
      "learning_rate": 4.8091634114583336e-05,
      "loss": 2.6543,
      "step": 470
    },
    {
      "epoch": 0.1171875,
      "grad_norm": 3.3092832565307617,
      "learning_rate": 4.8050944010416674e-05,
      "loss": 2.6949,
      "step": 480
    },
    {
      "epoch": 0.11962890625,
      "grad_norm": 3.2262418270111084,
      "learning_rate": 4.8010253906250005e-05,
      "loss": 2.6682,
      "step": 490
    },
    {
      "epoch": 0.1220703125,
      "grad_norm": 2.6953492164611816,
      "learning_rate": 4.7969563802083336e-05,
      "loss": 2.6201,
      "step": 500
    },
    {
      "epoch": 0.12451171875,
      "grad_norm": 4.662933826446533,
      "learning_rate": 4.7928873697916674e-05,
      "loss": 2.6257,
      "step": 510
    },
    {
      "epoch": 0.126953125,
      "grad_norm": 3.0636091232299805,
      "learning_rate": 4.7888183593750005e-05,
      "loss": 2.6415,
      "step": 520
    },
    {
      "epoch": 0.12939453125,
      "grad_norm": 3.6652345657348633,
      "learning_rate": 4.7847493489583336e-05,
      "loss": 2.4919,
      "step": 530
    },
    {
      "epoch": 0.1318359375,
      "grad_norm": 3.8242406845092773,
      "learning_rate": 4.7806803385416673e-05,
      "loss": 2.5863,
      "step": 540
    },
    {
      "epoch": 0.13427734375,
      "grad_norm": 4.141569137573242,
      "learning_rate": 4.7766113281250004e-05,
      "loss": 2.6849,
      "step": 550
    },
    {
      "epoch": 0.13671875,
      "grad_norm": 3.3883965015411377,
      "learning_rate": 4.7725423177083336e-05,
      "loss": 2.5733,
      "step": 560
    },
    {
      "epoch": 0.13916015625,
      "grad_norm": 2.574551820755005,
      "learning_rate": 4.768473307291667e-05,
      "loss": 2.6441,
      "step": 570
    },
    {
      "epoch": 0.1416015625,
      "grad_norm": 2.5890097618103027,
      "learning_rate": 4.7644042968750004e-05,
      "loss": 2.5634,
      "step": 580
    },
    {
      "epoch": 0.14404296875,
      "grad_norm": 2.8084876537323,
      "learning_rate": 4.7603352864583335e-05,
      "loss": 2.5957,
      "step": 590
    },
    {
      "epoch": 0.146484375,
      "grad_norm": 2.749541997909546,
      "learning_rate": 4.756266276041667e-05,
      "loss": 2.5682,
      "step": 600
    },
    {
      "epoch": 0.14892578125,
      "grad_norm": 2.8590869903564453,
      "learning_rate": 4.7521972656250004e-05,
      "loss": 2.6272,
      "step": 610
    },
    {
      "epoch": 0.1513671875,
      "grad_norm": 4.664872646331787,
      "learning_rate": 4.7481282552083335e-05,
      "loss": 2.7399,
      "step": 620
    },
    {
      "epoch": 0.15380859375,
      "grad_norm": 3.000126838684082,
      "learning_rate": 4.744059244791667e-05,
      "loss": 2.5081,
      "step": 630
    },
    {
      "epoch": 0.15625,
      "grad_norm": 3.271104574203491,
      "learning_rate": 4.7399902343750004e-05,
      "loss": 2.57,
      "step": 640
    },
    {
      "epoch": 0.15869140625,
      "grad_norm": 3.9172534942626953,
      "learning_rate": 4.7359212239583335e-05,
      "loss": 2.6152,
      "step": 650
    },
    {
      "epoch": 0.1611328125,
      "grad_norm": 4.524773120880127,
      "learning_rate": 4.731852213541667e-05,
      "loss": 2.6284,
      "step": 660
    },
    {
      "epoch": 0.16357421875,
      "grad_norm": 4.5355000495910645,
      "learning_rate": 4.7277832031250004e-05,
      "loss": 2.681,
      "step": 670
    },
    {
      "epoch": 0.166015625,
      "grad_norm": 3.421436309814453,
      "learning_rate": 4.7237141927083335e-05,
      "loss": 2.5072,
      "step": 680
    },
    {
      "epoch": 0.16845703125,
      "grad_norm": 2.4370713233947754,
      "learning_rate": 4.719645182291667e-05,
      "loss": 2.4443,
      "step": 690
    },
    {
      "epoch": 0.1708984375,
      "grad_norm": 3.273259401321411,
      "learning_rate": 4.7155761718750004e-05,
      "loss": 2.5706,
      "step": 700
    },
    {
      "epoch": 0.17333984375,
      "grad_norm": 2.6781065464019775,
      "learning_rate": 4.7115071614583335e-05,
      "loss": 2.4941,
      "step": 710
    },
    {
      "epoch": 0.17578125,
      "grad_norm": 3.317603349685669,
      "learning_rate": 4.707438151041667e-05,
      "loss": 2.4392,
      "step": 720
    },
    {
      "epoch": 0.17822265625,
      "grad_norm": 3.638306140899658,
      "learning_rate": 4.7033691406250004e-05,
      "loss": 2.6044,
      "step": 730
    },
    {
      "epoch": 0.1806640625,
      "grad_norm": 2.787062168121338,
      "learning_rate": 4.6993001302083335e-05,
      "loss": 2.6289,
      "step": 740
    },
    {
      "epoch": 0.18310546875,
      "grad_norm": 3.618781805038452,
      "learning_rate": 4.695231119791667e-05,
      "loss": 2.6206,
      "step": 750
    },
    {
      "epoch": 0.185546875,
      "grad_norm": 2.901409149169922,
      "learning_rate": 4.6911621093750004e-05,
      "loss": 2.4253,
      "step": 760
    },
    {
      "epoch": 0.18798828125,
      "grad_norm": 3.616349458694458,
      "learning_rate": 4.6870930989583335e-05,
      "loss": 2.4425,
      "step": 770
    },
    {
      "epoch": 0.1904296875,
      "grad_norm": 3.9460067749023438,
      "learning_rate": 4.683024088541667e-05,
      "loss": 2.5771,
      "step": 780
    },
    {
      "epoch": 0.19287109375,
      "grad_norm": 5.0386786460876465,
      "learning_rate": 4.6789550781250004e-05,
      "loss": 2.4632,
      "step": 790
    },
    {
      "epoch": 0.1953125,
      "grad_norm": 4.0897908210754395,
      "learning_rate": 4.6748860677083335e-05,
      "loss": 2.4634,
      "step": 800
    },
    {
      "epoch": 0.19775390625,
      "grad_norm": 3.3323378562927246,
      "learning_rate": 4.670817057291667e-05,
      "loss": 2.6204,
      "step": 810
    },
    {
      "epoch": 0.2001953125,
      "grad_norm": 2.6066603660583496,
      "learning_rate": 4.6667480468750004e-05,
      "loss": 2.3957,
      "step": 820
    },
    {
      "epoch": 0.20263671875,
      "grad_norm": 4.063283920288086,
      "learning_rate": 4.6626790364583335e-05,
      "loss": 2.5573,
      "step": 830
    },
    {
      "epoch": 0.205078125,
      "grad_norm": 4.175415515899658,
      "learning_rate": 4.658610026041667e-05,
      "loss": 2.7454,
      "step": 840
    },
    {
      "epoch": 0.20751953125,
      "grad_norm": 3.7000718116760254,
      "learning_rate": 4.6545410156250003e-05,
      "loss": 2.4601,
      "step": 850
    },
    {
      "epoch": 0.2099609375,
      "grad_norm": 3.775360584259033,
      "learning_rate": 4.6504720052083334e-05,
      "loss": 2.6121,
      "step": 860
    },
    {
      "epoch": 0.21240234375,
      "grad_norm": 4.406064510345459,
      "learning_rate": 4.646402994791667e-05,
      "loss": 2.5991,
      "step": 870
    },
    {
      "epoch": 0.21484375,
      "grad_norm": 2.625972032546997,
      "learning_rate": 4.642333984375e-05,
      "loss": 2.2812,
      "step": 880
    },
    {
      "epoch": 0.21728515625,
      "grad_norm": 4.459609031677246,
      "learning_rate": 4.6382649739583334e-05,
      "loss": 2.4061,
      "step": 890
    },
    {
      "epoch": 0.2197265625,
      "grad_norm": 2.933351516723633,
      "learning_rate": 4.634195963541667e-05,
      "loss": 2.4007,
      "step": 900
    },
    {
      "epoch": 0.22216796875,
      "grad_norm": 3.5763325691223145,
      "learning_rate": 4.630126953125e-05,
      "loss": 2.4292,
      "step": 910
    },
    {
      "epoch": 0.224609375,
      "grad_norm": 3.2205240726470947,
      "learning_rate": 4.6260579427083334e-05,
      "loss": 2.4544,
      "step": 920
    },
    {
      "epoch": 0.22705078125,
      "grad_norm": 4.992169380187988,
      "learning_rate": 4.621988932291667e-05,
      "loss": 2.462,
      "step": 930
    },
    {
      "epoch": 0.2294921875,
      "grad_norm": 4.042328357696533,
      "learning_rate": 4.617919921875e-05,
      "loss": 2.6096,
      "step": 940
    },
    {
      "epoch": 0.23193359375,
      "grad_norm": 3.2343802452087402,
      "learning_rate": 4.6138509114583334e-05,
      "loss": 2.5301,
      "step": 950
    },
    {
      "epoch": 0.234375,
      "grad_norm": 4.33451509475708,
      "learning_rate": 4.609781901041667e-05,
      "loss": 2.4489,
      "step": 960
    },
    {
      "epoch": 0.23681640625,
      "grad_norm": 2.6322290897369385,
      "learning_rate": 4.605712890625e-05,
      "loss": 2.4281,
      "step": 970
    },
    {
      "epoch": 0.2392578125,
      "grad_norm": 3.7243170738220215,
      "learning_rate": 4.6016438802083334e-05,
      "loss": 2.4729,
      "step": 980
    },
    {
      "epoch": 0.24169921875,
      "grad_norm": 5.234172344207764,
      "learning_rate": 4.597574869791667e-05,
      "loss": 2.4529,
      "step": 990
    },
    {
      "epoch": 0.244140625,
      "grad_norm": 4.067644119262695,
      "learning_rate": 4.593505859375e-05,
      "loss": 2.4559,
      "step": 1000
    },
    {
      "epoch": 0.24658203125,
      "grad_norm": 4.85115385055542,
      "learning_rate": 4.5894368489583334e-05,
      "loss": 2.5267,
      "step": 1010
    },
    {
      "epoch": 0.2490234375,
      "grad_norm": 2.9952774047851562,
      "learning_rate": 4.585367838541667e-05,
      "loss": 2.5077,
      "step": 1020
    },
    {
      "epoch": 0.25146484375,
      "grad_norm": 8.425843238830566,
      "learning_rate": 4.581298828125e-05,
      "loss": 2.5778,
      "step": 1030
    },
    {
      "epoch": 0.25390625,
      "grad_norm": 3.3884801864624023,
      "learning_rate": 4.5772298177083334e-05,
      "loss": 2.3779,
      "step": 1040
    },
    {
      "epoch": 0.25634765625,
      "grad_norm": 3.9245779514312744,
      "learning_rate": 4.573160807291667e-05,
      "loss": 2.475,
      "step": 1050
    },
    {
      "epoch": 0.2587890625,
      "grad_norm": 4.260089874267578,
      "learning_rate": 4.569091796875e-05,
      "loss": 2.5352,
      "step": 1060
    },
    {
      "epoch": 0.26123046875,
      "grad_norm": 3.535731554031372,
      "learning_rate": 4.5650227864583334e-05,
      "loss": 2.384,
      "step": 1070
    },
    {
      "epoch": 0.263671875,
      "grad_norm": 2.436676025390625,
      "learning_rate": 4.560953776041667e-05,
      "loss": 2.3242,
      "step": 1080
    },
    {
      "epoch": 0.26611328125,
      "grad_norm": 2.8076467514038086,
      "learning_rate": 4.556884765625e-05,
      "loss": 2.4175,
      "step": 1090
    },
    {
      "epoch": 0.2685546875,
      "grad_norm": 2.7620832920074463,
      "learning_rate": 4.5528157552083334e-05,
      "loss": 2.5319,
      "step": 1100
    },
    {
      "epoch": 0.27099609375,
      "grad_norm": 4.173006057739258,
      "learning_rate": 4.548746744791667e-05,
      "loss": 2.3411,
      "step": 1110
    },
    {
      "epoch": 0.2734375,
      "grad_norm": 4.005383491516113,
      "learning_rate": 4.544677734375e-05,
      "loss": 2.5685,
      "step": 1120
    },
    {
      "epoch": 0.27587890625,
      "grad_norm": 3.5556209087371826,
      "learning_rate": 4.5406087239583333e-05,
      "loss": 2.4006,
      "step": 1130
    },
    {
      "epoch": 0.2783203125,
      "grad_norm": 4.346380710601807,
      "learning_rate": 4.536539713541667e-05,
      "loss": 2.2395,
      "step": 1140
    },
    {
      "epoch": 0.28076171875,
      "grad_norm": 3.173506498336792,
      "learning_rate": 4.532470703125e-05,
      "loss": 2.4827,
      "step": 1150
    },
    {
      "epoch": 0.283203125,
      "grad_norm": 2.7906334400177,
      "learning_rate": 4.528401692708333e-05,
      "loss": 2.2658,
      "step": 1160
    },
    {
      "epoch": 0.28564453125,
      "grad_norm": 4.826812267303467,
      "learning_rate": 4.524332682291667e-05,
      "loss": 2.3376,
      "step": 1170
    },
    {
      "epoch": 0.2880859375,
      "grad_norm": 4.484918117523193,
      "learning_rate": 4.520263671875e-05,
      "loss": 2.416,
      "step": 1180
    },
    {
      "epoch": 0.29052734375,
      "grad_norm": 4.33822774887085,
      "learning_rate": 4.516194661458333e-05,
      "loss": 2.304,
      "step": 1190
    },
    {
      "epoch": 0.29296875,
      "grad_norm": 3.137972831726074,
      "learning_rate": 4.512125651041667e-05,
      "loss": 2.303,
      "step": 1200
    },
    {
      "epoch": 0.29541015625,
      "grad_norm": 3.2417731285095215,
      "learning_rate": 4.508056640625e-05,
      "loss": 2.4214,
      "step": 1210
    },
    {
      "epoch": 0.2978515625,
      "grad_norm": 5.048618793487549,
      "learning_rate": 4.503987630208333e-05,
      "loss": 2.4885,
      "step": 1220
    },
    {
      "epoch": 0.30029296875,
      "grad_norm": 3.740194320678711,
      "learning_rate": 4.499918619791667e-05,
      "loss": 2.4118,
      "step": 1230
    },
    {
      "epoch": 0.302734375,
      "grad_norm": 4.431063175201416,
      "learning_rate": 4.495849609375e-05,
      "loss": 2.5511,
      "step": 1240
    },
    {
      "epoch": 0.30517578125,
      "grad_norm": 3.1024725437164307,
      "learning_rate": 4.491780598958333e-05,
      "loss": 2.3771,
      "step": 1250
    },
    {
      "epoch": 0.3076171875,
      "grad_norm": 4.626742362976074,
      "learning_rate": 4.487711588541667e-05,
      "loss": 2.4741,
      "step": 1260
    },
    {
      "epoch": 0.31005859375,
      "grad_norm": 4.30294132232666,
      "learning_rate": 4.483642578125e-05,
      "loss": 2.3905,
      "step": 1270
    },
    {
      "epoch": 0.3125,
      "grad_norm": 4.873531341552734,
      "learning_rate": 4.479573567708333e-05,
      "loss": 2.4132,
      "step": 1280
    },
    {
      "epoch": 0.31494140625,
      "grad_norm": 3.8076281547546387,
      "learning_rate": 4.475504557291667e-05,
      "loss": 2.3189,
      "step": 1290
    },
    {
      "epoch": 0.3173828125,
      "grad_norm": 4.243046283721924,
      "learning_rate": 4.471435546875e-05,
      "loss": 2.2949,
      "step": 1300
    },
    {
      "epoch": 0.31982421875,
      "grad_norm": 2.7318224906921387,
      "learning_rate": 4.467366536458333e-05,
      "loss": 2.3971,
      "step": 1310
    },
    {
      "epoch": 0.322265625,
      "grad_norm": 3.2198920249938965,
      "learning_rate": 4.463297526041667e-05,
      "loss": 2.3944,
      "step": 1320
    },
    {
      "epoch": 0.32470703125,
      "grad_norm": 4.190115451812744,
      "learning_rate": 4.459228515625e-05,
      "loss": 2.3119,
      "step": 1330
    },
    {
      "epoch": 0.3271484375,
      "grad_norm": 5.157310485839844,
      "learning_rate": 4.455159505208333e-05,
      "loss": 2.433,
      "step": 1340
    },
    {
      "epoch": 0.32958984375,
      "grad_norm": 3.2606327533721924,
      "learning_rate": 4.451090494791667e-05,
      "loss": 2.2756,
      "step": 1350
    },
    {
      "epoch": 0.33203125,
      "grad_norm": 4.251391887664795,
      "learning_rate": 4.447021484375e-05,
      "loss": 2.496,
      "step": 1360
    },
    {
      "epoch": 0.33447265625,
      "grad_norm": 4.0941619873046875,
      "learning_rate": 4.442952473958333e-05,
      "loss": 2.373,
      "step": 1370
    },
    {
      "epoch": 0.3369140625,
      "grad_norm": 2.885021209716797,
      "learning_rate": 4.438883463541667e-05,
      "loss": 2.2834,
      "step": 1380
    },
    {
      "epoch": 0.33935546875,
      "grad_norm": 3.2341434955596924,
      "learning_rate": 4.434814453125e-05,
      "loss": 2.3684,
      "step": 1390
    },
    {
      "epoch": 0.341796875,
      "grad_norm": 3.997642755508423,
      "learning_rate": 4.430745442708333e-05,
      "loss": 2.3602,
      "step": 1400
    },
    {
      "epoch": 0.34423828125,
      "grad_norm": 3.9857442378997803,
      "learning_rate": 4.426676432291667e-05,
      "loss": 2.2266,
      "step": 1410
    },
    {
      "epoch": 0.3466796875,
      "grad_norm": 4.37493896484375,
      "learning_rate": 4.422607421875e-05,
      "loss": 2.2952,
      "step": 1420
    },
    {
      "epoch": 0.34912109375,
      "grad_norm": 4.590832233428955,
      "learning_rate": 4.418538411458333e-05,
      "loss": 2.1577,
      "step": 1430
    },
    {
      "epoch": 0.3515625,
      "grad_norm": 3.7318766117095947,
      "learning_rate": 4.414469401041667e-05,
      "loss": 2.4118,
      "step": 1440
    },
    {
      "epoch": 0.35400390625,
      "grad_norm": 3.2162461280822754,
      "learning_rate": 4.410400390625e-05,
      "loss": 2.2895,
      "step": 1450
    },
    {
      "epoch": 0.3564453125,
      "grad_norm": 3.803744316101074,
      "learning_rate": 4.406331380208333e-05,
      "loss": 2.1877,
      "step": 1460
    },
    {
      "epoch": 0.35888671875,
      "grad_norm": 3.3482930660247803,
      "learning_rate": 4.402262369791667e-05,
      "loss": 2.4171,
      "step": 1470
    },
    {
      "epoch": 0.361328125,
      "grad_norm": 3.0993199348449707,
      "learning_rate": 4.398193359375e-05,
      "loss": 2.3444,
      "step": 1480
    },
    {
      "epoch": 0.36376953125,
      "grad_norm": 3.758544445037842,
      "learning_rate": 4.394124348958333e-05,
      "loss": 2.2615,
      "step": 1490
    },
    {
      "epoch": 0.3662109375,
      "grad_norm": 5.640161514282227,
      "learning_rate": 4.390055338541667e-05,
      "loss": 2.3482,
      "step": 1500
    },
    {
      "epoch": 0.36865234375,
      "grad_norm": 4.901711940765381,
      "learning_rate": 4.385986328125e-05,
      "loss": 2.3735,
      "step": 1510
    },
    {
      "epoch": 0.37109375,
      "grad_norm": 5.361192226409912,
      "learning_rate": 4.381917317708333e-05,
      "loss": 2.3537,
      "step": 1520
    },
    {
      "epoch": 0.37353515625,
      "grad_norm": 3.233325242996216,
      "learning_rate": 4.377848307291667e-05,
      "loss": 2.2383,
      "step": 1530
    },
    {
      "epoch": 0.3759765625,
      "grad_norm": 4.206266403198242,
      "learning_rate": 4.373779296875e-05,
      "loss": 2.2774,
      "step": 1540
    },
    {
      "epoch": 0.37841796875,
      "grad_norm": 3.7397680282592773,
      "learning_rate": 4.369710286458333e-05,
      "loss": 2.3127,
      "step": 1550
    },
    {
      "epoch": 0.380859375,
      "grad_norm": 3.1373093128204346,
      "learning_rate": 4.365641276041667e-05,
      "loss": 2.2365,
      "step": 1560
    },
    {
      "epoch": 0.38330078125,
      "grad_norm": 3.2202627658843994,
      "learning_rate": 4.361572265625e-05,
      "loss": 2.2461,
      "step": 1570
    },
    {
      "epoch": 0.3857421875,
      "grad_norm": 4.185499668121338,
      "learning_rate": 4.357503255208333e-05,
      "loss": 2.2813,
      "step": 1580
    },
    {
      "epoch": 0.38818359375,
      "grad_norm": 3.8223769664764404,
      "learning_rate": 4.353434244791667e-05,
      "loss": 2.3459,
      "step": 1590
    },
    {
      "epoch": 0.390625,
      "grad_norm": 5.381956577301025,
      "learning_rate": 4.349365234375e-05,
      "loss": 2.4222,
      "step": 1600
    },
    {
      "epoch": 0.39306640625,
      "grad_norm": 4.312890529632568,
      "learning_rate": 4.345296223958333e-05,
      "loss": 2.5022,
      "step": 1610
    },
    {
      "epoch": 0.3955078125,
      "grad_norm": 2.6339616775512695,
      "learning_rate": 4.341227213541667e-05,
      "loss": 2.2979,
      "step": 1620
    },
    {
      "epoch": 0.39794921875,
      "grad_norm": 3.0974152088165283,
      "learning_rate": 4.337158203125e-05,
      "loss": 2.3895,
      "step": 1630
    },
    {
      "epoch": 0.400390625,
      "grad_norm": 4.18810510635376,
      "learning_rate": 4.333089192708333e-05,
      "loss": 2.3587,
      "step": 1640
    },
    {
      "epoch": 0.40283203125,
      "grad_norm": 3.9539976119995117,
      "learning_rate": 4.329020182291667e-05,
      "loss": 2.2669,
      "step": 1650
    },
    {
      "epoch": 0.4052734375,
      "grad_norm": 4.938467025756836,
      "learning_rate": 4.324951171875e-05,
      "loss": 2.4516,
      "step": 1660
    },
    {
      "epoch": 0.40771484375,
      "grad_norm": 3.5643558502197266,
      "learning_rate": 4.320882161458333e-05,
      "loss": 2.3309,
      "step": 1670
    },
    {
      "epoch": 0.41015625,
      "grad_norm": 3.5164148807525635,
      "learning_rate": 4.316813151041667e-05,
      "loss": 2.1399,
      "step": 1680
    },
    {
      "epoch": 0.41259765625,
      "grad_norm": 4.1942291259765625,
      "learning_rate": 4.312744140625e-05,
      "loss": 2.1653,
      "step": 1690
    },
    {
      "epoch": 0.4150390625,
      "grad_norm": 2.893385887145996,
      "learning_rate": 4.308675130208333e-05,
      "loss": 2.2184,
      "step": 1700
    },
    {
      "epoch": 0.41748046875,
      "grad_norm": 4.739438056945801,
      "learning_rate": 4.304606119791667e-05,
      "loss": 2.3466,
      "step": 1710
    },
    {
      "epoch": 0.419921875,
      "grad_norm": 3.934777021408081,
      "learning_rate": 4.300537109375e-05,
      "loss": 2.4287,
      "step": 1720
    },
    {
      "epoch": 0.42236328125,
      "grad_norm": 3.676433563232422,
      "learning_rate": 4.296468098958333e-05,
      "loss": 2.1787,
      "step": 1730
    },
    {
      "epoch": 0.4248046875,
      "grad_norm": 6.29453706741333,
      "learning_rate": 4.292399088541667e-05,
      "loss": 2.2918,
      "step": 1740
    },
    {
      "epoch": 0.42724609375,
      "grad_norm": 5.148534297943115,
      "learning_rate": 4.288330078125e-05,
      "loss": 2.2945,
      "step": 1750
    },
    {
      "epoch": 0.4296875,
      "grad_norm": 4.162693500518799,
      "learning_rate": 4.284261067708333e-05,
      "loss": 2.2055,
      "step": 1760
    },
    {
      "epoch": 0.43212890625,
      "grad_norm": 6.21742057800293,
      "learning_rate": 4.280192057291667e-05,
      "loss": 2.371,
      "step": 1770
    },
    {
      "epoch": 0.4345703125,
      "grad_norm": 3.127270460128784,
      "learning_rate": 4.276123046875e-05,
      "loss": 2.2359,
      "step": 1780
    },
    {
      "epoch": 0.43701171875,
      "grad_norm": 4.009226322174072,
      "learning_rate": 4.272054036458333e-05,
      "loss": 2.2048,
      "step": 1790
    },
    {
      "epoch": 0.439453125,
      "grad_norm": 2.737962484359741,
      "learning_rate": 4.267985026041667e-05,
      "loss": 2.3709,
      "step": 1800
    },
    {
      "epoch": 0.44189453125,
      "grad_norm": 3.3276684284210205,
      "learning_rate": 4.263916015625e-05,
      "loss": 2.1823,
      "step": 1810
    },
    {
      "epoch": 0.4443359375,
      "grad_norm": 3.599008083343506,
      "learning_rate": 4.259847005208333e-05,
      "loss": 2.4759,
      "step": 1820
    },
    {
      "epoch": 0.44677734375,
      "grad_norm": 5.386082172393799,
      "learning_rate": 4.255777994791667e-05,
      "loss": 2.2846,
      "step": 1830
    },
    {
      "epoch": 0.44921875,
      "grad_norm": 4.9569854736328125,
      "learning_rate": 4.251708984375e-05,
      "loss": 2.2874,
      "step": 1840
    },
    {
      "epoch": 0.45166015625,
      "grad_norm": 5.273927688598633,
      "learning_rate": 4.247639973958333e-05,
      "loss": 2.4703,
      "step": 1850
    },
    {
      "epoch": 0.4541015625,
      "grad_norm": 5.458299160003662,
      "learning_rate": 4.243570963541667e-05,
      "loss": 2.3203,
      "step": 1860
    },
    {
      "epoch": 0.45654296875,
      "grad_norm": 4.305429458618164,
      "learning_rate": 4.239501953125e-05,
      "loss": 2.2615,
      "step": 1870
    },
    {
      "epoch": 0.458984375,
      "grad_norm": 6.538087844848633,
      "learning_rate": 4.235432942708333e-05,
      "loss": 2.3846,
      "step": 1880
    },
    {
      "epoch": 0.46142578125,
      "grad_norm": 4.022402286529541,
      "learning_rate": 4.231363932291667e-05,
      "loss": 2.4186,
      "step": 1890
    },
    {
      "epoch": 0.4638671875,
      "grad_norm": 4.783799648284912,
      "learning_rate": 4.227294921875e-05,
      "loss": 2.2335,
      "step": 1900
    },
    {
      "epoch": 0.46630859375,
      "grad_norm": 5.6971235275268555,
      "learning_rate": 4.223225911458333e-05,
      "loss": 2.3266,
      "step": 1910
    },
    {
      "epoch": 0.46875,
      "grad_norm": 5.251561641693115,
      "learning_rate": 4.219156901041667e-05,
      "loss": 2.206,
      "step": 1920
    },
    {
      "epoch": 0.47119140625,
      "grad_norm": 4.546469688415527,
      "learning_rate": 4.215087890625e-05,
      "loss": 2.3293,
      "step": 1930
    },
    {
      "epoch": 0.4736328125,
      "grad_norm": 5.636441707611084,
      "learning_rate": 4.211018880208333e-05,
      "loss": 2.4969,
      "step": 1940
    },
    {
      "epoch": 0.47607421875,
      "grad_norm": 4.480672359466553,
      "learning_rate": 4.206949869791667e-05,
      "loss": 2.2425,
      "step": 1950
    },
    {
      "epoch": 0.478515625,
      "grad_norm": 3.175841808319092,
      "learning_rate": 4.202880859375e-05,
      "loss": 2.2117,
      "step": 1960
    },
    {
      "epoch": 0.48095703125,
      "grad_norm": 5.517563343048096,
      "learning_rate": 4.198811848958333e-05,
      "loss": 2.2121,
      "step": 1970
    },
    {
      "epoch": 0.4833984375,
      "grad_norm": 6.438080310821533,
      "learning_rate": 4.194742838541667e-05,
      "loss": 2.289,
      "step": 1980
    },
    {
      "epoch": 0.48583984375,
      "grad_norm": 4.674368381500244,
      "learning_rate": 4.190673828125e-05,
      "loss": 2.3352,
      "step": 1990
    },
    {
      "epoch": 0.48828125,
      "grad_norm": 6.128015041351318,
      "learning_rate": 4.186604817708333e-05,
      "loss": 2.347,
      "step": 2000
    },
    {
      "epoch": 0.49072265625,
      "grad_norm": 4.481894016265869,
      "learning_rate": 4.182535807291667e-05,
      "loss": 2.128,
      "step": 2010
    },
    {
      "epoch": 0.4931640625,
      "grad_norm": 3.1363418102264404,
      "learning_rate": 4.178466796875e-05,
      "loss": 2.2566,
      "step": 2020
    },
    {
      "epoch": 0.49560546875,
      "grad_norm": 4.187832832336426,
      "learning_rate": 4.174397786458333e-05,
      "loss": 2.1598,
      "step": 2030
    },
    {
      "epoch": 0.498046875,
      "grad_norm": 4.472349643707275,
      "learning_rate": 4.170328776041667e-05,
      "loss": 2.295,
      "step": 2040
    },
    {
      "epoch": 0.50048828125,
      "grad_norm": 5.041716575622559,
      "learning_rate": 4.166259765625e-05,
      "loss": 2.369,
      "step": 2050
    },
    {
      "epoch": 0.5029296875,
      "grad_norm": 5.2125563621521,
      "learning_rate": 4.162190755208333e-05,
      "loss": 2.2577,
      "step": 2060
    },
    {
      "epoch": 0.50537109375,
      "grad_norm": 3.2715048789978027,
      "learning_rate": 4.158121744791667e-05,
      "loss": 2.3527,
      "step": 2070
    },
    {
      "epoch": 0.5078125,
      "grad_norm": 6.2797722816467285,
      "learning_rate": 4.154052734375e-05,
      "loss": 2.0964,
      "step": 2080
    },
    {
      "epoch": 0.51025390625,
      "grad_norm": 4.406488418579102,
      "learning_rate": 4.149983723958334e-05,
      "loss": 2.2625,
      "step": 2090
    },
    {
      "epoch": 0.5126953125,
      "grad_norm": 4.177445888519287,
      "learning_rate": 4.145914713541667e-05,
      "loss": 2.239,
      "step": 2100
    },
    {
      "epoch": 0.51513671875,
      "grad_norm": 4.642061233520508,
      "learning_rate": 4.141845703125e-05,
      "loss": 2.2904,
      "step": 2110
    },
    {
      "epoch": 0.517578125,
      "grad_norm": 4.0497260093688965,
      "learning_rate": 4.137776692708334e-05,
      "loss": 2.2184,
      "step": 2120
    },
    {
      "epoch": 0.52001953125,
      "grad_norm": 5.331879615783691,
      "learning_rate": 4.133707682291667e-05,
      "loss": 2.3433,
      "step": 2130
    },
    {
      "epoch": 0.5224609375,
      "grad_norm": 3.9486100673675537,
      "learning_rate": 4.129638671875e-05,
      "loss": 2.1976,
      "step": 2140
    },
    {
      "epoch": 0.52490234375,
      "grad_norm": 3.3370156288146973,
      "learning_rate": 4.1255696614583337e-05,
      "loss": 2.3111,
      "step": 2150
    },
    {
      "epoch": 0.52734375,
      "grad_norm": 3.794982671737671,
      "learning_rate": 4.121500651041667e-05,
      "loss": 2.4081,
      "step": 2160
    },
    {
      "epoch": 0.52978515625,
      "grad_norm": 10.442627906799316,
      "learning_rate": 4.117431640625e-05,
      "loss": 2.1894,
      "step": 2170
    },
    {
      "epoch": 0.5322265625,
      "grad_norm": 4.717621803283691,
      "learning_rate": 4.1133626302083336e-05,
      "loss": 2.457,
      "step": 2180
    },
    {
      "epoch": 0.53466796875,
      "grad_norm": 4.300724029541016,
      "learning_rate": 4.109293619791667e-05,
      "loss": 2.2129,
      "step": 2190
    },
    {
      "epoch": 0.537109375,
      "grad_norm": 2.7332801818847656,
      "learning_rate": 4.1052246093750005e-05,
      "loss": 2.2629,
      "step": 2200
    },
    {
      "epoch": 0.53955078125,
      "grad_norm": 3.179892063140869,
      "learning_rate": 4.1011555989583336e-05,
      "loss": 2.1969,
      "step": 2210
    },
    {
      "epoch": 0.5419921875,
      "grad_norm": 3.7057018280029297,
      "learning_rate": 4.097086588541667e-05,
      "loss": 2.2691,
      "step": 2220
    },
    {
      "epoch": 0.54443359375,
      "grad_norm": 4.121400833129883,
      "learning_rate": 4.0930175781250005e-05,
      "loss": 2.3186,
      "step": 2230
    },
    {
      "epoch": 0.546875,
      "grad_norm": 5.929062366485596,
      "learning_rate": 4.0889485677083336e-05,
      "loss": 2.2598,
      "step": 2240
    },
    {
      "epoch": 0.54931640625,
      "grad_norm": 7.541342735290527,
      "learning_rate": 4.084879557291667e-05,
      "loss": 2.2633,
      "step": 2250
    },
    {
      "epoch": 0.5517578125,
      "grad_norm": 3.0202813148498535,
      "learning_rate": 4.0808105468750005e-05,
      "loss": 2.2585,
      "step": 2260
    },
    {
      "epoch": 0.55419921875,
      "grad_norm": 4.114500522613525,
      "learning_rate": 4.0767415364583336e-05,
      "loss": 2.1857,
      "step": 2270
    },
    {
      "epoch": 0.556640625,
      "grad_norm": 4.378981113433838,
      "learning_rate": 4.072672526041667e-05,
      "loss": 2.2819,
      "step": 2280
    },
    {
      "epoch": 0.55908203125,
      "grad_norm": 9.613456726074219,
      "learning_rate": 4.0686035156250005e-05,
      "loss": 2.1599,
      "step": 2290
    },
    {
      "epoch": 0.5615234375,
      "grad_norm": 3.293562173843384,
      "learning_rate": 4.0645345052083336e-05,
      "loss": 2.3219,
      "step": 2300
    },
    {
      "epoch": 0.56396484375,
      "grad_norm": 3.278207302093506,
      "learning_rate": 4.0604654947916674e-05,
      "loss": 2.1095,
      "step": 2310
    },
    {
      "epoch": 0.56640625,
      "grad_norm": 4.654114246368408,
      "learning_rate": 4.0563964843750005e-05,
      "loss": 2.1805,
      "step": 2320
    },
    {
      "epoch": 0.56884765625,
      "grad_norm": 4.38844108581543,
      "learning_rate": 4.0523274739583336e-05,
      "loss": 2.0583,
      "step": 2330
    },
    {
      "epoch": 0.5712890625,
      "grad_norm": 6.624650001525879,
      "learning_rate": 4.0482584635416674e-05,
      "loss": 2.104,
      "step": 2340
    },
    {
      "epoch": 0.57373046875,
      "grad_norm": 3.3899502754211426,
      "learning_rate": 4.0441894531250005e-05,
      "loss": 2.2469,
      "step": 2350
    },
    {
      "epoch": 0.576171875,
      "grad_norm": 5.446058750152588,
      "learning_rate": 4.0401204427083336e-05,
      "loss": 2.1871,
      "step": 2360
    },
    {
      "epoch": 0.57861328125,
      "grad_norm": 3.341209888458252,
      "learning_rate": 4.0360514322916674e-05,
      "loss": 2.1966,
      "step": 2370
    },
    {
      "epoch": 0.5810546875,
      "grad_norm": 3.0768191814422607,
      "learning_rate": 4.0319824218750005e-05,
      "loss": 2.1498,
      "step": 2380
    },
    {
      "epoch": 0.58349609375,
      "grad_norm": 5.950117588043213,
      "learning_rate": 4.0279134114583336e-05,
      "loss": 2.3468,
      "step": 2390
    },
    {
      "epoch": 0.5859375,
      "grad_norm": 3.6580774784088135,
      "learning_rate": 4.0238444010416673e-05,
      "loss": 2.2553,
      "step": 2400
    },
    {
      "epoch": 0.58837890625,
      "grad_norm": 4.305599689483643,
      "learning_rate": 4.0197753906250005e-05,
      "loss": 2.2608,
      "step": 2410
    },
    {
      "epoch": 0.5908203125,
      "grad_norm": 3.7245490550994873,
      "learning_rate": 4.0157063802083336e-05,
      "loss": 2.1678,
      "step": 2420
    },
    {
      "epoch": 0.59326171875,
      "grad_norm": 3.6587107181549072,
      "learning_rate": 4.011637369791667e-05,
      "loss": 2.2574,
      "step": 2430
    },
    {
      "epoch": 0.595703125,
      "grad_norm": 3.164916515350342,
      "learning_rate": 4.0075683593750004e-05,
      "loss": 2.2964,
      "step": 2440
    },
    {
      "epoch": 0.59814453125,
      "grad_norm": 5.118406772613525,
      "learning_rate": 4.0034993489583335e-05,
      "loss": 2.1723,
      "step": 2450
    },
    {
      "epoch": 0.6005859375,
      "grad_norm": 4.183587074279785,
      "learning_rate": 3.999430338541667e-05,
      "loss": 2.1987,
      "step": 2460
    },
    {
      "epoch": 0.60302734375,
      "grad_norm": 4.668349742889404,
      "learning_rate": 3.9953613281250004e-05,
      "loss": 2.2058,
      "step": 2470
    },
    {
      "epoch": 0.60546875,
      "grad_norm": 4.157285213470459,
      "learning_rate": 3.9912923177083335e-05,
      "loss": 2.2605,
      "step": 2480
    },
    {
      "epoch": 0.60791015625,
      "grad_norm": 6.334705352783203,
      "learning_rate": 3.987223307291667e-05,
      "loss": 2.2722,
      "step": 2490
    },
    {
      "epoch": 0.6103515625,
      "grad_norm": 3.9874963760375977,
      "learning_rate": 3.9831542968750004e-05,
      "loss": 2.2603,
      "step": 2500
    },
    {
      "epoch": 0.61279296875,
      "grad_norm": 4.3937811851501465,
      "learning_rate": 3.9790852864583335e-05,
      "loss": 2.2263,
      "step": 2510
    },
    {
      "epoch": 0.615234375,
      "grad_norm": 2.8510141372680664,
      "learning_rate": 3.975016276041667e-05,
      "loss": 2.1617,
      "step": 2520
    },
    {
      "epoch": 0.61767578125,
      "grad_norm": 5.1816606521606445,
      "learning_rate": 3.9709472656250004e-05,
      "loss": 2.2118,
      "step": 2530
    },
    {
      "epoch": 0.6201171875,
      "grad_norm": 6.418839454650879,
      "learning_rate": 3.9668782552083335e-05,
      "loss": 2.2026,
      "step": 2540
    },
    {
      "epoch": 0.62255859375,
      "grad_norm": 5.48510217666626,
      "learning_rate": 3.962809244791667e-05,
      "loss": 2.1476,
      "step": 2550
    },
    {
      "epoch": 0.625,
      "grad_norm": 4.744391441345215,
      "learning_rate": 3.9587402343750004e-05,
      "loss": 2.1789,
      "step": 2560
    },
    {
      "epoch": 0.62744140625,
      "grad_norm": 4.899561405181885,
      "learning_rate": 3.9546712239583335e-05,
      "loss": 2.2453,
      "step": 2570
    },
    {
      "epoch": 0.6298828125,
      "grad_norm": 4.0859856605529785,
      "learning_rate": 3.950602213541667e-05,
      "loss": 2.4793,
      "step": 2580
    },
    {
      "epoch": 0.63232421875,
      "grad_norm": 3.478109836578369,
      "learning_rate": 3.9465332031250004e-05,
      "loss": 2.2195,
      "step": 2590
    },
    {
      "epoch": 0.634765625,
      "grad_norm": 4.4444780349731445,
      "learning_rate": 3.9424641927083335e-05,
      "loss": 2.2175,
      "step": 2600
    },
    {
      "epoch": 0.63720703125,
      "grad_norm": 4.168842315673828,
      "learning_rate": 3.938395182291667e-05,
      "loss": 2.171,
      "step": 2610
    },
    {
      "epoch": 0.6396484375,
      "grad_norm": 5.17594051361084,
      "learning_rate": 3.9343261718750004e-05,
      "loss": 2.0929,
      "step": 2620
    },
    {
      "epoch": 0.64208984375,
      "grad_norm": 5.35209321975708,
      "learning_rate": 3.9302571614583335e-05,
      "loss": 2.2314,
      "step": 2630
    },
    {
      "epoch": 0.64453125,
      "grad_norm": 4.389188289642334,
      "learning_rate": 3.926188151041667e-05,
      "loss": 2.3291,
      "step": 2640
    },
    {
      "epoch": 0.64697265625,
      "grad_norm": 4.352789402008057,
      "learning_rate": 3.9221191406250004e-05,
      "loss": 2.3279,
      "step": 2650
    },
    {
      "epoch": 0.6494140625,
      "grad_norm": 3.9913041591644287,
      "learning_rate": 3.9180501302083335e-05,
      "loss": 2.3121,
      "step": 2660
    },
    {
      "epoch": 0.65185546875,
      "grad_norm": 3.8205270767211914,
      "learning_rate": 3.913981119791667e-05,
      "loss": 2.1226,
      "step": 2670
    },
    {
      "epoch": 0.654296875,
      "grad_norm": 4.688061237335205,
      "learning_rate": 3.9099121093750004e-05,
      "loss": 2.0314,
      "step": 2680
    },
    {
      "epoch": 0.65673828125,
      "grad_norm": 5.060595512390137,
      "learning_rate": 3.9058430989583335e-05,
      "loss": 2.2341,
      "step": 2690
    },
    {
      "epoch": 0.6591796875,
      "grad_norm": 3.474451780319214,
      "learning_rate": 3.901774088541667e-05,
      "loss": 2.0844,
      "step": 2700
    },
    {
      "epoch": 0.66162109375,
      "grad_norm": 3.591005802154541,
      "learning_rate": 3.8977050781250003e-05,
      "loss": 2.2275,
      "step": 2710
    },
    {
      "epoch": 0.6640625,
      "grad_norm": 4.881007671356201,
      "learning_rate": 3.8936360677083334e-05,
      "loss": 2.3484,
      "step": 2720
    },
    {
      "epoch": 0.66650390625,
      "grad_norm": 3.1513900756835938,
      "learning_rate": 3.889567057291667e-05,
      "loss": 2.3097,
      "step": 2730
    },
    {
      "epoch": 0.6689453125,
      "grad_norm": 5.611917018890381,
      "learning_rate": 3.885498046875e-05,
      "loss": 2.1602,
      "step": 2740
    },
    {
      "epoch": 0.67138671875,
      "grad_norm": 5.966181755065918,
      "learning_rate": 3.8814290364583334e-05,
      "loss": 2.1738,
      "step": 2750
    },
    {
      "epoch": 0.673828125,
      "grad_norm": 2.9838345050811768,
      "learning_rate": 3.877360026041667e-05,
      "loss": 2.0111,
      "step": 2760
    },
    {
      "epoch": 0.67626953125,
      "grad_norm": 5.065608978271484,
      "learning_rate": 3.873291015625e-05,
      "loss": 2.0979,
      "step": 2770
    },
    {
      "epoch": 0.6787109375,
      "grad_norm": 3.587405204772949,
      "learning_rate": 3.8692220052083334e-05,
      "loss": 2.1716,
      "step": 2780
    },
    {
      "epoch": 0.68115234375,
      "grad_norm": 6.587690830230713,
      "learning_rate": 3.865152994791667e-05,
      "loss": 2.1842,
      "step": 2790
    },
    {
      "epoch": 0.68359375,
      "grad_norm": 5.384626388549805,
      "learning_rate": 3.861083984375e-05,
      "loss": 2.1497,
      "step": 2800
    },
    {
      "epoch": 0.68603515625,
      "grad_norm": 4.665221691131592,
      "learning_rate": 3.8570149739583334e-05,
      "loss": 2.1123,
      "step": 2810
    },
    {
      "epoch": 0.6884765625,
      "grad_norm": 4.783215522766113,
      "learning_rate": 3.852945963541667e-05,
      "loss": 2.2428,
      "step": 2820
    },
    {
      "epoch": 0.69091796875,
      "grad_norm": 5.029446601867676,
      "learning_rate": 3.848876953125e-05,
      "loss": 2.1399,
      "step": 2830
    },
    {
      "epoch": 0.693359375,
      "grad_norm": 5.000881671905518,
      "learning_rate": 3.8448079427083334e-05,
      "loss": 2.3144,
      "step": 2840
    },
    {
      "epoch": 0.69580078125,
      "grad_norm": 5.57279634475708,
      "learning_rate": 3.840738932291667e-05,
      "loss": 2.3172,
      "step": 2850
    },
    {
      "epoch": 0.6982421875,
      "grad_norm": 3.108701467514038,
      "learning_rate": 3.836669921875e-05,
      "loss": 2.1766,
      "step": 2860
    },
    {
      "epoch": 0.70068359375,
      "grad_norm": 9.7427339553833,
      "learning_rate": 3.8326009114583334e-05,
      "loss": 2.1951,
      "step": 2870
    },
    {
      "epoch": 0.703125,
      "grad_norm": 7.21509313583374,
      "learning_rate": 3.828531901041667e-05,
      "loss": 2.1744,
      "step": 2880
    },
    {
      "epoch": 0.70556640625,
      "grad_norm": 6.521020889282227,
      "learning_rate": 3.824462890625e-05,
      "loss": 2.2276,
      "step": 2890
    },
    {
      "epoch": 0.7080078125,
      "grad_norm": 4.1323323249816895,
      "learning_rate": 3.8203938802083334e-05,
      "loss": 2.2922,
      "step": 2900
    },
    {
      "epoch": 0.71044921875,
      "grad_norm": 5.8207550048828125,
      "learning_rate": 3.816324869791667e-05,
      "loss": 2.2003,
      "step": 2910
    },
    {
      "epoch": 0.712890625,
      "grad_norm": 4.40871000289917,
      "learning_rate": 3.812255859375e-05,
      "loss": 2.2226,
      "step": 2920
    },
    {
      "epoch": 0.71533203125,
      "grad_norm": 4.401834011077881,
      "learning_rate": 3.8081868489583334e-05,
      "loss": 2.1687,
      "step": 2930
    },
    {
      "epoch": 0.7177734375,
      "grad_norm": 4.4840803146362305,
      "learning_rate": 3.804117838541667e-05,
      "loss": 2.214,
      "step": 2940
    },
    {
      "epoch": 0.72021484375,
      "grad_norm": 3.3971638679504395,
      "learning_rate": 3.800048828125e-05,
      "loss": 2.2069,
      "step": 2950
    },
    {
      "epoch": 0.72265625,
      "grad_norm": 6.358722686767578,
      "learning_rate": 3.7959798177083334e-05,
      "loss": 2.2573,
      "step": 2960
    },
    {
      "epoch": 0.72509765625,
      "grad_norm": 4.934082984924316,
      "learning_rate": 3.791910807291667e-05,
      "loss": 2.0198,
      "step": 2970
    },
    {
      "epoch": 0.7275390625,
      "grad_norm": 5.066126346588135,
      "learning_rate": 3.787841796875e-05,
      "loss": 2.0521,
      "step": 2980
    },
    {
      "epoch": 0.72998046875,
      "grad_norm": 5.325676441192627,
      "learning_rate": 3.7837727864583334e-05,
      "loss": 2.117,
      "step": 2990
    },
    {
      "epoch": 0.732421875,
      "grad_norm": 6.024717807769775,
      "learning_rate": 3.779703776041667e-05,
      "loss": 2.1539,
      "step": 3000
    },
    {
      "epoch": 0.73486328125,
      "grad_norm": 3.5623385906219482,
      "learning_rate": 3.775634765625e-05,
      "loss": 2.1416,
      "step": 3010
    },
    {
      "epoch": 0.7373046875,
      "grad_norm": 3.145648241043091,
      "learning_rate": 3.7715657552083333e-05,
      "loss": 2.1695,
      "step": 3020
    },
    {
      "epoch": 0.73974609375,
      "grad_norm": 3.090339183807373,
      "learning_rate": 3.767496744791667e-05,
      "loss": 2.3221,
      "step": 3030
    },
    {
      "epoch": 0.7421875,
      "grad_norm": 27.92289924621582,
      "learning_rate": 3.763427734375e-05,
      "loss": 2.1789,
      "step": 3040
    },
    {
      "epoch": 0.74462890625,
      "grad_norm": 3.7147738933563232,
      "learning_rate": 3.759358723958333e-05,
      "loss": 2.2425,
      "step": 3050
    },
    {
      "epoch": 0.7470703125,
      "grad_norm": 3.782149076461792,
      "learning_rate": 3.755289713541667e-05,
      "loss": 2.2728,
      "step": 3060
    },
    {
      "epoch": 0.74951171875,
      "grad_norm": 6.253957271575928,
      "learning_rate": 3.751220703125e-05,
      "loss": 2.2202,
      "step": 3070
    },
    {
      "epoch": 0.751953125,
      "grad_norm": 2.848924160003662,
      "learning_rate": 3.747151692708333e-05,
      "loss": 2.1315,
      "step": 3080
    },
    {
      "epoch": 0.75439453125,
      "grad_norm": 4.092543601989746,
      "learning_rate": 3.743082682291667e-05,
      "loss": 2.0926,
      "step": 3090
    },
    {
      "epoch": 0.7568359375,
      "grad_norm": 3.3388564586639404,
      "learning_rate": 3.739013671875e-05,
      "loss": 2.3541,
      "step": 3100
    },
    {
      "epoch": 0.75927734375,
      "grad_norm": 4.49321174621582,
      "learning_rate": 3.734944661458333e-05,
      "loss": 2.1087,
      "step": 3110
    },
    {
      "epoch": 0.76171875,
      "grad_norm": 5.623506546020508,
      "learning_rate": 3.730875651041667e-05,
      "loss": 2.3521,
      "step": 3120
    },
    {
      "epoch": 0.76416015625,
      "grad_norm": 5.257645606994629,
      "learning_rate": 3.726806640625e-05,
      "loss": 2.2004,
      "step": 3130
    },
    {
      "epoch": 0.7666015625,
      "grad_norm": 5.450674533843994,
      "learning_rate": 3.722737630208333e-05,
      "loss": 2.2942,
      "step": 3140
    },
    {
      "epoch": 0.76904296875,
      "grad_norm": 6.960068702697754,
      "learning_rate": 3.718668619791667e-05,
      "loss": 2.0174,
      "step": 3150
    },
    {
      "epoch": 0.771484375,
      "grad_norm": 3.741514205932617,
      "learning_rate": 3.714599609375e-05,
      "loss": 2.2608,
      "step": 3160
    },
    {
      "epoch": 0.77392578125,
      "grad_norm": 4.4867987632751465,
      "learning_rate": 3.710530598958333e-05,
      "loss": 2.1633,
      "step": 3170
    },
    {
      "epoch": 0.7763671875,
      "grad_norm": 4.757977485656738,
      "learning_rate": 3.706461588541667e-05,
      "loss": 2.1373,
      "step": 3180
    },
    {
      "epoch": 0.77880859375,
      "grad_norm": 4.34169340133667,
      "learning_rate": 3.702392578125e-05,
      "loss": 2.206,
      "step": 3190
    },
    {
      "epoch": 0.78125,
      "grad_norm": 4.4586710929870605,
      "learning_rate": 3.698323567708333e-05,
      "loss": 2.1859,
      "step": 3200
    },
    {
      "epoch": 0.78369140625,
      "grad_norm": 5.9385199546813965,
      "learning_rate": 3.694254557291667e-05,
      "loss": 2.2591,
      "step": 3210
    },
    {
      "epoch": 0.7861328125,
      "grad_norm": 3.879901885986328,
      "learning_rate": 3.690185546875e-05,
      "loss": 2.3066,
      "step": 3220
    },
    {
      "epoch": 0.78857421875,
      "grad_norm": 3.324845790863037,
      "learning_rate": 3.686116536458333e-05,
      "loss": 2.237,
      "step": 3230
    },
    {
      "epoch": 0.791015625,
      "grad_norm": 4.663888454437256,
      "learning_rate": 3.682047526041667e-05,
      "loss": 2.0428,
      "step": 3240
    },
    {
      "epoch": 0.79345703125,
      "grad_norm": 5.063830852508545,
      "learning_rate": 3.677978515625e-05,
      "loss": 2.2421,
      "step": 3250
    },
    {
      "epoch": 0.7958984375,
      "grad_norm": 6.267895698547363,
      "learning_rate": 3.673909505208333e-05,
      "loss": 2.2802,
      "step": 3260
    },
    {
      "epoch": 0.79833984375,
      "grad_norm": 5.0084004402160645,
      "learning_rate": 3.669840494791667e-05,
      "loss": 2.3413,
      "step": 3270
    },
    {
      "epoch": 0.80078125,
      "grad_norm": 3.5080907344818115,
      "learning_rate": 3.665771484375e-05,
      "loss": 2.0885,
      "step": 3280
    },
    {
      "epoch": 0.80322265625,
      "grad_norm": 9.139204978942871,
      "learning_rate": 3.661702473958333e-05,
      "loss": 2.0299,
      "step": 3290
    },
    {
      "epoch": 0.8056640625,
      "grad_norm": 3.6423773765563965,
      "learning_rate": 3.657633463541667e-05,
      "loss": 2.1943,
      "step": 3300
    },
    {
      "epoch": 0.80810546875,
      "grad_norm": 3.393260955810547,
      "learning_rate": 3.653564453125e-05,
      "loss": 2.1098,
      "step": 3310
    },
    {
      "epoch": 0.810546875,
      "grad_norm": 3.699103832244873,
      "learning_rate": 3.649495442708333e-05,
      "loss": 2.1047,
      "step": 3320
    },
    {
      "epoch": 0.81298828125,
      "grad_norm": 3.5507290363311768,
      "learning_rate": 3.645426432291667e-05,
      "loss": 2.0128,
      "step": 3330
    },
    {
      "epoch": 0.8154296875,
      "grad_norm": 6.36447811126709,
      "learning_rate": 3.641357421875e-05,
      "loss": 2.0638,
      "step": 3340
    },
    {
      "epoch": 0.81787109375,
      "grad_norm": 5.161458969116211,
      "learning_rate": 3.637288411458333e-05,
      "loss": 2.2096,
      "step": 3350
    },
    {
      "epoch": 0.8203125,
      "grad_norm": 5.063691139221191,
      "learning_rate": 3.633219401041667e-05,
      "loss": 2.2147,
      "step": 3360
    },
    {
      "epoch": 0.82275390625,
      "grad_norm": 5.834591865539551,
      "learning_rate": 3.629150390625e-05,
      "loss": 2.0973,
      "step": 3370
    },
    {
      "epoch": 0.8251953125,
      "grad_norm": 6.031013488769531,
      "learning_rate": 3.625081380208333e-05,
      "loss": 2.2273,
      "step": 3380
    },
    {
      "epoch": 0.82763671875,
      "grad_norm": 6.33578634262085,
      "learning_rate": 3.621012369791667e-05,
      "loss": 2.0633,
      "step": 3390
    },
    {
      "epoch": 0.830078125,
      "grad_norm": 4.717619895935059,
      "learning_rate": 3.616943359375e-05,
      "loss": 2.1431,
      "step": 3400
    },
    {
      "epoch": 0.83251953125,
      "grad_norm": 4.640944480895996,
      "learning_rate": 3.612874348958333e-05,
      "loss": 1.9731,
      "step": 3410
    },
    {
      "epoch": 0.8349609375,
      "grad_norm": 4.6094794273376465,
      "learning_rate": 3.608805338541667e-05,
      "loss": 2.0818,
      "step": 3420
    },
    {
      "epoch": 0.83740234375,
      "grad_norm": 3.801938772201538,
      "learning_rate": 3.604736328125e-05,
      "loss": 2.2969,
      "step": 3430
    },
    {
      "epoch": 0.83984375,
      "grad_norm": 6.196681976318359,
      "learning_rate": 3.600667317708333e-05,
      "loss": 2.0828,
      "step": 3440
    },
    {
      "epoch": 0.84228515625,
      "grad_norm": 3.9990711212158203,
      "learning_rate": 3.596598307291667e-05,
      "loss": 2.1483,
      "step": 3450
    },
    {
      "epoch": 0.8447265625,
      "grad_norm": 5.113337516784668,
      "learning_rate": 3.592529296875e-05,
      "loss": 2.2009,
      "step": 3460
    },
    {
      "epoch": 0.84716796875,
      "grad_norm": 6.915909767150879,
      "learning_rate": 3.588460286458333e-05,
      "loss": 1.9684,
      "step": 3470
    },
    {
      "epoch": 0.849609375,
      "grad_norm": 3.867478609085083,
      "learning_rate": 3.584391276041667e-05,
      "loss": 2.138,
      "step": 3480
    },
    {
      "epoch": 0.85205078125,
      "grad_norm": 4.640645980834961,
      "learning_rate": 3.580322265625e-05,
      "loss": 2.2373,
      "step": 3490
    },
    {
      "epoch": 0.8544921875,
      "grad_norm": 4.413156986236572,
      "learning_rate": 3.576253255208333e-05,
      "loss": 2.1029,
      "step": 3500
    },
    {
      "epoch": 0.85693359375,
      "grad_norm": 4.740275859832764,
      "learning_rate": 3.572184244791667e-05,
      "loss": 2.1075,
      "step": 3510
    },
    {
      "epoch": 0.859375,
      "grad_norm": 3.8169405460357666,
      "learning_rate": 3.568115234375e-05,
      "loss": 2.1331,
      "step": 3520
    },
    {
      "epoch": 0.86181640625,
      "grad_norm": 3.7936785221099854,
      "learning_rate": 3.564046223958333e-05,
      "loss": 2.3131,
      "step": 3530
    },
    {
      "epoch": 0.8642578125,
      "grad_norm": 3.4978995323181152,
      "learning_rate": 3.559977213541667e-05,
      "loss": 2.0951,
      "step": 3540
    },
    {
      "epoch": 0.86669921875,
      "grad_norm": 3.1837656497955322,
      "learning_rate": 3.555908203125e-05,
      "loss": 2.1596,
      "step": 3550
    },
    {
      "epoch": 0.869140625,
      "grad_norm": 5.252554416656494,
      "learning_rate": 3.551839192708333e-05,
      "loss": 2.1149,
      "step": 3560
    },
    {
      "epoch": 0.87158203125,
      "grad_norm": 3.6734752655029297,
      "learning_rate": 3.547770182291667e-05,
      "loss": 2.2824,
      "step": 3570
    },
    {
      "epoch": 0.8740234375,
      "grad_norm": 6.335815906524658,
      "learning_rate": 3.543701171875e-05,
      "loss": 2.1295,
      "step": 3580
    },
    {
      "epoch": 0.87646484375,
      "grad_norm": 5.732814311981201,
      "learning_rate": 3.539632161458333e-05,
      "loss": 2.1313,
      "step": 3590
    },
    {
      "epoch": 0.87890625,
      "grad_norm": 5.301650524139404,
      "learning_rate": 3.535563151041667e-05,
      "loss": 1.9362,
      "step": 3600
    },
    {
      "epoch": 0.88134765625,
      "grad_norm": 5.708104133605957,
      "learning_rate": 3.531494140625e-05,
      "loss": 2.2614,
      "step": 3610
    },
    {
      "epoch": 0.8837890625,
      "grad_norm": 7.454988956451416,
      "learning_rate": 3.527425130208333e-05,
      "loss": 2.1217,
      "step": 3620
    },
    {
      "epoch": 0.88623046875,
      "grad_norm": 4.481109619140625,
      "learning_rate": 3.523356119791667e-05,
      "loss": 2.0507,
      "step": 3630
    },
    {
      "epoch": 0.888671875,
      "grad_norm": 6.744863510131836,
      "learning_rate": 3.519287109375e-05,
      "loss": 2.0632,
      "step": 3640
    },
    {
      "epoch": 0.89111328125,
      "grad_norm": 6.207267761230469,
      "learning_rate": 3.515218098958333e-05,
      "loss": 2.2013,
      "step": 3650
    },
    {
      "epoch": 0.8935546875,
      "grad_norm": 4.761909484863281,
      "learning_rate": 3.511149088541667e-05,
      "loss": 2.0815,
      "step": 3660
    },
    {
      "epoch": 0.89599609375,
      "grad_norm": 4.49261474609375,
      "learning_rate": 3.507080078125e-05,
      "loss": 2.0693,
      "step": 3670
    },
    {
      "epoch": 0.8984375,
      "grad_norm": 3.4024899005889893,
      "learning_rate": 3.503011067708333e-05,
      "loss": 2.0909,
      "step": 3680
    },
    {
      "epoch": 0.90087890625,
      "grad_norm": 9.927433013916016,
      "learning_rate": 3.498942057291667e-05,
      "loss": 2.1449,
      "step": 3690
    },
    {
      "epoch": 0.9033203125,
      "grad_norm": 4.30161714553833,
      "learning_rate": 3.494873046875e-05,
      "loss": 2.2376,
      "step": 3700
    },
    {
      "epoch": 0.90576171875,
      "grad_norm": 4.864509105682373,
      "learning_rate": 3.490804036458333e-05,
      "loss": 2.2135,
      "step": 3710
    },
    {
      "epoch": 0.908203125,
      "grad_norm": 4.728153705596924,
      "learning_rate": 3.486735026041667e-05,
      "loss": 2.1566,
      "step": 3720
    },
    {
      "epoch": 0.91064453125,
      "grad_norm": 3.1447842121124268,
      "learning_rate": 3.482666015625e-05,
      "loss": 2.2341,
      "step": 3730
    },
    {
      "epoch": 0.9130859375,
      "grad_norm": 4.915918827056885,
      "learning_rate": 3.478597005208333e-05,
      "loss": 2.0027,
      "step": 3740
    },
    {
      "epoch": 0.91552734375,
      "grad_norm": 4.440026760101318,
      "learning_rate": 3.474527994791667e-05,
      "loss": 2.0665,
      "step": 3750
    },
    {
      "epoch": 0.91796875,
      "grad_norm": 3.5994951725006104,
      "learning_rate": 3.470458984375e-05,
      "loss": 2.1653,
      "step": 3760
    },
    {
      "epoch": 0.92041015625,
      "grad_norm": 6.040307521820068,
      "learning_rate": 3.466389973958333e-05,
      "loss": 2.1155,
      "step": 3770
    },
    {
      "epoch": 0.9228515625,
      "grad_norm": 5.62387228012085,
      "learning_rate": 3.462320963541667e-05,
      "loss": 1.9778,
      "step": 3780
    },
    {
      "epoch": 0.92529296875,
      "grad_norm": 5.737608909606934,
      "learning_rate": 3.458251953125e-05,
      "loss": 2.3031,
      "step": 3790
    },
    {
      "epoch": 0.927734375,
      "grad_norm": 5.758780479431152,
      "learning_rate": 3.454182942708333e-05,
      "loss": 2.0435,
      "step": 3800
    },
    {
      "epoch": 0.93017578125,
      "grad_norm": 3.6696183681488037,
      "learning_rate": 3.450113932291667e-05,
      "loss": 2.1572,
      "step": 3810
    },
    {
      "epoch": 0.9326171875,
      "grad_norm": 4.6943678855896,
      "learning_rate": 3.446044921875e-05,
      "loss": 2.0847,
      "step": 3820
    },
    {
      "epoch": 0.93505859375,
      "grad_norm": 4.598230361938477,
      "learning_rate": 3.441975911458333e-05,
      "loss": 2.1817,
      "step": 3830
    },
    {
      "epoch": 0.9375,
      "grad_norm": 4.806694507598877,
      "learning_rate": 3.437906901041667e-05,
      "loss": 2.0064,
      "step": 3840
    },
    {
      "epoch": 0.93994140625,
      "grad_norm": 7.736335754394531,
      "learning_rate": 3.433837890625e-05,
      "loss": 2.2516,
      "step": 3850
    },
    {
      "epoch": 0.9423828125,
      "grad_norm": 4.6642608642578125,
      "learning_rate": 3.429768880208333e-05,
      "loss": 2.0814,
      "step": 3860
    },
    {
      "epoch": 0.94482421875,
      "grad_norm": 5.846436023712158,
      "learning_rate": 3.425699869791667e-05,
      "loss": 2.1826,
      "step": 3870
    },
    {
      "epoch": 0.947265625,
      "grad_norm": 5.21744966506958,
      "learning_rate": 3.421630859375e-05,
      "loss": 2.0133,
      "step": 3880
    },
    {
      "epoch": 0.94970703125,
      "grad_norm": 5.280106067657471,
      "learning_rate": 3.417561848958333e-05,
      "loss": 2.1866,
      "step": 3890
    },
    {
      "epoch": 0.9521484375,
      "grad_norm": 5.741605758666992,
      "learning_rate": 3.413492838541667e-05,
      "loss": 2.1214,
      "step": 3900
    },
    {
      "epoch": 0.95458984375,
      "grad_norm": 5.530792713165283,
      "learning_rate": 3.409423828125e-05,
      "loss": 2.2807,
      "step": 3910
    },
    {
      "epoch": 0.95703125,
      "grad_norm": 9.218859672546387,
      "learning_rate": 3.405354817708333e-05,
      "loss": 2.1369,
      "step": 3920
    },
    {
      "epoch": 0.95947265625,
      "grad_norm": 4.879312038421631,
      "learning_rate": 3.401285807291667e-05,
      "loss": 1.9584,
      "step": 3930
    },
    {
      "epoch": 0.9619140625,
      "grad_norm": 4.633406162261963,
      "learning_rate": 3.397216796875e-05,
      "loss": 2.0468,
      "step": 3940
    },
    {
      "epoch": 0.96435546875,
      "grad_norm": 3.875065326690674,
      "learning_rate": 3.393147786458333e-05,
      "loss": 2.094,
      "step": 3950
    },
    {
      "epoch": 0.966796875,
      "grad_norm": 3.939108371734619,
      "learning_rate": 3.389078776041667e-05,
      "loss": 2.198,
      "step": 3960
    },
    {
      "epoch": 0.96923828125,
      "grad_norm": 5.754821300506592,
      "learning_rate": 3.385009765625e-05,
      "loss": 2.0244,
      "step": 3970
    },
    {
      "epoch": 0.9716796875,
      "grad_norm": 4.794145584106445,
      "learning_rate": 3.380940755208333e-05,
      "loss": 2.3666,
      "step": 3980
    },
    {
      "epoch": 0.97412109375,
      "grad_norm": 3.461228370666504,
      "learning_rate": 3.376871744791667e-05,
      "loss": 2.2397,
      "step": 3990
    },
    {
      "epoch": 0.9765625,
      "grad_norm": 4.0164899826049805,
      "learning_rate": 3.372802734375e-05,
      "loss": 2.02,
      "step": 4000
    },
    {
      "epoch": 0.97900390625,
      "grad_norm": 4.2218098640441895,
      "learning_rate": 3.368733723958333e-05,
      "loss": 2.2889,
      "step": 4010
    },
    {
      "epoch": 0.9814453125,
      "grad_norm": 5.062053203582764,
      "learning_rate": 3.364664713541667e-05,
      "loss": 2.1271,
      "step": 4020
    },
    {
      "epoch": 0.98388671875,
      "grad_norm": 4.2571611404418945,
      "learning_rate": 3.360595703125e-05,
      "loss": 2.2047,
      "step": 4030
    },
    {
      "epoch": 0.986328125,
      "grad_norm": 6.016940593719482,
      "learning_rate": 3.356526692708333e-05,
      "loss": 2.0711,
      "step": 4040
    },
    {
      "epoch": 0.98876953125,
      "grad_norm": 3.2832133769989014,
      "learning_rate": 3.352457682291667e-05,
      "loss": 2.0811,
      "step": 4050
    },
    {
      "epoch": 0.9912109375,
      "grad_norm": 5.559938907623291,
      "learning_rate": 3.348388671875e-05,
      "loss": 2.0595,
      "step": 4060
    },
    {
      "epoch": 0.99365234375,
      "grad_norm": 5.996962547302246,
      "learning_rate": 3.3443196614583336e-05,
      "loss": 1.9211,
      "step": 4070
    },
    {
      "epoch": 0.99609375,
      "grad_norm": 6.16412353515625,
      "learning_rate": 3.340250651041667e-05,
      "loss": 2.0618,
      "step": 4080
    },
    {
      "epoch": 0.99853515625,
      "grad_norm": 5.4455156326293945,
      "learning_rate": 3.336181640625e-05,
      "loss": 2.0812,
      "step": 4090
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9336681365966797,
      "eval_runtime": 5.1813,
      "eval_samples_per_second": 24.704,
      "eval_steps_per_second": 24.704,
      "step": 4096
    },
    {
      "epoch": 1.0009765625,
      "grad_norm": 3.603276014328003,
      "learning_rate": 3.3321126302083336e-05,
      "loss": 2.0267,
      "step": 4100
    },
    {
      "epoch": 1.00341796875,
      "grad_norm": 4.841551780700684,
      "learning_rate": 3.328043619791667e-05,
      "loss": 2.1907,
      "step": 4110
    },
    {
      "epoch": 1.005859375,
      "grad_norm": 3.8613440990448,
      "learning_rate": 3.323974609375e-05,
      "loss": 2.0827,
      "step": 4120
    },
    {
      "epoch": 1.00830078125,
      "grad_norm": 6.3251118659973145,
      "learning_rate": 3.3199055989583336e-05,
      "loss": 2.0697,
      "step": 4130
    },
    {
      "epoch": 1.0107421875,
      "grad_norm": 4.662758827209473,
      "learning_rate": 3.315836588541667e-05,
      "loss": 2.0727,
      "step": 4140
    },
    {
      "epoch": 1.01318359375,
      "grad_norm": 4.216070175170898,
      "learning_rate": 3.311767578125e-05,
      "loss": 1.7457,
      "step": 4150
    },
    {
      "epoch": 1.015625,
      "grad_norm": 5.814503192901611,
      "learning_rate": 3.3076985677083336e-05,
      "loss": 2.1862,
      "step": 4160
    },
    {
      "epoch": 1.01806640625,
      "grad_norm": 4.152307510375977,
      "learning_rate": 3.303629557291667e-05,
      "loss": 2.0528,
      "step": 4170
    },
    {
      "epoch": 1.0205078125,
      "grad_norm": 5.738770008087158,
      "learning_rate": 3.2995605468750005e-05,
      "loss": 2.0904,
      "step": 4180
    },
    {
      "epoch": 1.02294921875,
      "grad_norm": 5.653615951538086,
      "learning_rate": 3.2954915364583336e-05,
      "loss": 2.0969,
      "step": 4190
    },
    {
      "epoch": 1.025390625,
      "grad_norm": 5.402867794036865,
      "learning_rate": 3.291422526041667e-05,
      "loss": 2.0198,
      "step": 4200
    },
    {
      "epoch": 1.02783203125,
      "grad_norm": 6.094822883605957,
      "learning_rate": 3.2873535156250005e-05,
      "loss": 2.224,
      "step": 4210
    },
    {
      "epoch": 1.0302734375,
      "grad_norm": 5.0826497077941895,
      "learning_rate": 3.2832845052083336e-05,
      "loss": 2.0331,
      "step": 4220
    },
    {
      "epoch": 1.03271484375,
      "grad_norm": 4.078188419342041,
      "learning_rate": 3.279215494791667e-05,
      "loss": 2.256,
      "step": 4230
    },
    {
      "epoch": 1.03515625,
      "grad_norm": 4.003580570220947,
      "learning_rate": 3.2751464843750005e-05,
      "loss": 2.1206,
      "step": 4240
    },
    {
      "epoch": 1.03759765625,
      "grad_norm": 4.912353038787842,
      "learning_rate": 3.2710774739583336e-05,
      "loss": 2.1008,
      "step": 4250
    },
    {
      "epoch": 1.0400390625,
      "grad_norm": 5.114283561706543,
      "learning_rate": 3.267008463541667e-05,
      "loss": 2.1747,
      "step": 4260
    },
    {
      "epoch": 1.04248046875,
      "grad_norm": 5.813259601593018,
      "learning_rate": 3.2629394531250005e-05,
      "loss": 2.2128,
      "step": 4270
    },
    {
      "epoch": 1.044921875,
      "grad_norm": 4.126799583435059,
      "learning_rate": 3.2588704427083336e-05,
      "loss": 2.2268,
      "step": 4280
    },
    {
      "epoch": 1.04736328125,
      "grad_norm": 3.391134262084961,
      "learning_rate": 3.2548014322916673e-05,
      "loss": 1.9434,
      "step": 4290
    },
    {
      "epoch": 1.0498046875,
      "grad_norm": 3.434833526611328,
      "learning_rate": 3.2507324218750004e-05,
      "loss": 2.0458,
      "step": 4300
    },
    {
      "epoch": 1.05224609375,
      "grad_norm": 7.567718029022217,
      "learning_rate": 3.2466634114583336e-05,
      "loss": 2.009,
      "step": 4310
    },
    {
      "epoch": 1.0546875,
      "grad_norm": 6.187688827514648,
      "learning_rate": 3.242594401041667e-05,
      "loss": 2.138,
      "step": 4320
    },
    {
      "epoch": 1.05712890625,
      "grad_norm": 4.327930927276611,
      "learning_rate": 3.2385253906250004e-05,
      "loss": 1.8501,
      "step": 4330
    },
    {
      "epoch": 1.0595703125,
      "grad_norm": 3.0845136642456055,
      "learning_rate": 3.2344563802083335e-05,
      "loss": 2.0979,
      "step": 4340
    },
    {
      "epoch": 1.06201171875,
      "grad_norm": 5.395917892456055,
      "learning_rate": 3.230387369791667e-05,
      "loss": 1.9662,
      "step": 4350
    },
    {
      "epoch": 1.064453125,
      "grad_norm": 4.016072750091553,
      "learning_rate": 3.2263183593750004e-05,
      "loss": 2.0894,
      "step": 4360
    },
    {
      "epoch": 1.06689453125,
      "grad_norm": 4.264444351196289,
      "learning_rate": 3.2222493489583335e-05,
      "loss": 2.1666,
      "step": 4370
    },
    {
      "epoch": 1.0693359375,
      "grad_norm": 5.8941969871521,
      "learning_rate": 3.218180338541667e-05,
      "loss": 2.1253,
      "step": 4380
    },
    {
      "epoch": 1.07177734375,
      "grad_norm": 4.544514179229736,
      "learning_rate": 3.2141113281250004e-05,
      "loss": 2.1303,
      "step": 4390
    },
    {
      "epoch": 1.07421875,
      "grad_norm": 5.493311405181885,
      "learning_rate": 3.2100423177083335e-05,
      "loss": 2.213,
      "step": 4400
    },
    {
      "epoch": 1.07666015625,
      "grad_norm": 3.510209560394287,
      "learning_rate": 3.205973307291667e-05,
      "loss": 2.0585,
      "step": 4410
    },
    {
      "epoch": 1.0791015625,
      "grad_norm": 5.50161075592041,
      "learning_rate": 3.2019042968750004e-05,
      "loss": 2.0214,
      "step": 4420
    },
    {
      "epoch": 1.08154296875,
      "grad_norm": 5.414432525634766,
      "learning_rate": 3.1978352864583335e-05,
      "loss": 1.9975,
      "step": 4430
    },
    {
      "epoch": 1.083984375,
      "grad_norm": 5.390693664550781,
      "learning_rate": 3.193766276041667e-05,
      "loss": 2.074,
      "step": 4440
    },
    {
      "epoch": 1.08642578125,
      "grad_norm": 3.6662650108337402,
      "learning_rate": 3.1896972656250004e-05,
      "loss": 2.1499,
      "step": 4450
    },
    {
      "epoch": 1.0888671875,
      "grad_norm": 4.981851577758789,
      "learning_rate": 3.1856282552083335e-05,
      "loss": 1.9967,
      "step": 4460
    },
    {
      "epoch": 1.09130859375,
      "grad_norm": 5.9880547523498535,
      "learning_rate": 3.181559244791667e-05,
      "loss": 2.1167,
      "step": 4470
    },
    {
      "epoch": 1.09375,
      "grad_norm": 5.870184898376465,
      "learning_rate": 3.1774902343750004e-05,
      "loss": 2.0129,
      "step": 4480
    },
    {
      "epoch": 1.09619140625,
      "grad_norm": 3.5599420070648193,
      "learning_rate": 3.1734212239583335e-05,
      "loss": 2.2064,
      "step": 4490
    },
    {
      "epoch": 1.0986328125,
      "grad_norm": 6.388201713562012,
      "learning_rate": 3.169352213541667e-05,
      "loss": 1.9715,
      "step": 4500
    },
    {
      "epoch": 1.10107421875,
      "grad_norm": 3.7678279876708984,
      "learning_rate": 3.1652832031250004e-05,
      "loss": 1.9677,
      "step": 4510
    },
    {
      "epoch": 1.103515625,
      "grad_norm": 5.2800703048706055,
      "learning_rate": 3.1612141927083335e-05,
      "loss": 1.935,
      "step": 4520
    },
    {
      "epoch": 1.10595703125,
      "grad_norm": 4.358583450317383,
      "learning_rate": 3.157145182291667e-05,
      "loss": 2.0407,
      "step": 4530
    },
    {
      "epoch": 1.1083984375,
      "grad_norm": 6.714741230010986,
      "learning_rate": 3.1530761718750004e-05,
      "loss": 2.0928,
      "step": 4540
    },
    {
      "epoch": 1.11083984375,
      "grad_norm": 5.08919620513916,
      "learning_rate": 3.1490071614583335e-05,
      "loss": 2.0207,
      "step": 4550
    },
    {
      "epoch": 1.11328125,
      "grad_norm": 4.963147163391113,
      "learning_rate": 3.144938151041667e-05,
      "loss": 2.1647,
      "step": 4560
    },
    {
      "epoch": 1.11572265625,
      "grad_norm": 5.371307849884033,
      "learning_rate": 3.1408691406250004e-05,
      "loss": 2.1995,
      "step": 4570
    },
    {
      "epoch": 1.1181640625,
      "grad_norm": 7.315490245819092,
      "learning_rate": 3.1368001302083335e-05,
      "loss": 2.0735,
      "step": 4580
    },
    {
      "epoch": 1.12060546875,
      "grad_norm": 5.31577730178833,
      "learning_rate": 3.132731119791667e-05,
      "loss": 2.0127,
      "step": 4590
    },
    {
      "epoch": 1.123046875,
      "grad_norm": 5.002362251281738,
      "learning_rate": 3.1286621093750003e-05,
      "loss": 1.9262,
      "step": 4600
    },
    {
      "epoch": 1.12548828125,
      "grad_norm": 5.612344741821289,
      "learning_rate": 3.1245930989583334e-05,
      "loss": 2.058,
      "step": 4610
    },
    {
      "epoch": 1.1279296875,
      "grad_norm": 3.1973302364349365,
      "learning_rate": 3.120524088541667e-05,
      "loss": 2.2841,
      "step": 4620
    },
    {
      "epoch": 1.13037109375,
      "grad_norm": 6.750687599182129,
      "learning_rate": 3.116455078125e-05,
      "loss": 2.2868,
      "step": 4630
    },
    {
      "epoch": 1.1328125,
      "grad_norm": 4.637699604034424,
      "learning_rate": 3.1123860677083334e-05,
      "loss": 2.0164,
      "step": 4640
    },
    {
      "epoch": 1.13525390625,
      "grad_norm": 3.249396562576294,
      "learning_rate": 3.108317057291667e-05,
      "loss": 2.0117,
      "step": 4650
    },
    {
      "epoch": 1.1376953125,
      "grad_norm": 4.630910396575928,
      "learning_rate": 3.104248046875e-05,
      "loss": 2.0231,
      "step": 4660
    },
    {
      "epoch": 1.14013671875,
      "grad_norm": 3.18746280670166,
      "learning_rate": 3.1001790364583334e-05,
      "loss": 2.193,
      "step": 4670
    },
    {
      "epoch": 1.142578125,
      "grad_norm": 3.7627205848693848,
      "learning_rate": 3.096110026041667e-05,
      "loss": 2.0394,
      "step": 4680
    },
    {
      "epoch": 1.14501953125,
      "grad_norm": 2.7325875759124756,
      "learning_rate": 3.092041015625e-05,
      "loss": 1.9506,
      "step": 4690
    },
    {
      "epoch": 1.1474609375,
      "grad_norm": 5.181583404541016,
      "learning_rate": 3.0879720052083334e-05,
      "loss": 2.0568,
      "step": 4700
    },
    {
      "epoch": 1.14990234375,
      "grad_norm": 5.155348300933838,
      "learning_rate": 3.083902994791667e-05,
      "loss": 2.138,
      "step": 4710
    },
    {
      "epoch": 1.15234375,
      "grad_norm": 3.8980178833007812,
      "learning_rate": 3.079833984375e-05,
      "loss": 2.1171,
      "step": 4720
    },
    {
      "epoch": 1.15478515625,
      "grad_norm": 4.7451300621032715,
      "learning_rate": 3.0757649739583334e-05,
      "loss": 2.0032,
      "step": 4730
    },
    {
      "epoch": 1.1572265625,
      "grad_norm": 8.254284858703613,
      "learning_rate": 3.071695963541667e-05,
      "loss": 2.2806,
      "step": 4740
    },
    {
      "epoch": 1.15966796875,
      "grad_norm": 4.563650131225586,
      "learning_rate": 3.067626953125e-05,
      "loss": 2.1593,
      "step": 4750
    },
    {
      "epoch": 1.162109375,
      "grad_norm": 3.65466046333313,
      "learning_rate": 3.0635579427083334e-05,
      "loss": 2.2589,
      "step": 4760
    },
    {
      "epoch": 1.16455078125,
      "grad_norm": 8.10348892211914,
      "learning_rate": 3.059488932291667e-05,
      "loss": 2.2351,
      "step": 4770
    },
    {
      "epoch": 1.1669921875,
      "grad_norm": 5.726166725158691,
      "learning_rate": 3.055419921875e-05,
      "loss": 2.0478,
      "step": 4780
    },
    {
      "epoch": 1.16943359375,
      "grad_norm": 4.8315558433532715,
      "learning_rate": 3.0513509114583334e-05,
      "loss": 2.2024,
      "step": 4790
    },
    {
      "epoch": 1.171875,
      "grad_norm": 5.539503574371338,
      "learning_rate": 3.047281901041667e-05,
      "loss": 2.1157,
      "step": 4800
    },
    {
      "epoch": 1.17431640625,
      "grad_norm": 5.195261478424072,
      "learning_rate": 3.0432128906250003e-05,
      "loss": 2.0978,
      "step": 4810
    },
    {
      "epoch": 1.1767578125,
      "grad_norm": 5.545809745788574,
      "learning_rate": 3.0391438802083334e-05,
      "loss": 1.9666,
      "step": 4820
    },
    {
      "epoch": 1.17919921875,
      "grad_norm": 3.509498119354248,
      "learning_rate": 3.035074869791667e-05,
      "loss": 2.0148,
      "step": 4830
    },
    {
      "epoch": 1.181640625,
      "grad_norm": 6.109277725219727,
      "learning_rate": 3.0310058593750003e-05,
      "loss": 2.3702,
      "step": 4840
    },
    {
      "epoch": 1.18408203125,
      "grad_norm": 5.039164066314697,
      "learning_rate": 3.0269368489583334e-05,
      "loss": 2.1903,
      "step": 4850
    },
    {
      "epoch": 1.1865234375,
      "grad_norm": 4.044723987579346,
      "learning_rate": 3.022867838541667e-05,
      "loss": 2.0881,
      "step": 4860
    },
    {
      "epoch": 1.18896484375,
      "grad_norm": 5.85633659362793,
      "learning_rate": 3.0187988281250002e-05,
      "loss": 2.0611,
      "step": 4870
    },
    {
      "epoch": 1.19140625,
      "grad_norm": 4.203633785247803,
      "learning_rate": 3.0147298177083333e-05,
      "loss": 1.9721,
      "step": 4880
    },
    {
      "epoch": 1.19384765625,
      "grad_norm": 5.578654766082764,
      "learning_rate": 3.010660807291667e-05,
      "loss": 1.9486,
      "step": 4890
    },
    {
      "epoch": 1.1962890625,
      "grad_norm": 3.6967129707336426,
      "learning_rate": 3.0065917968750002e-05,
      "loss": 2.0998,
      "step": 4900
    },
    {
      "epoch": 1.19873046875,
      "grad_norm": 5.6538166999816895,
      "learning_rate": 3.0025227864583333e-05,
      "loss": 1.9988,
      "step": 4910
    },
    {
      "epoch": 1.201171875,
      "grad_norm": 4.7243242263793945,
      "learning_rate": 2.998453776041667e-05,
      "loss": 2.1983,
      "step": 4920
    },
    {
      "epoch": 1.20361328125,
      "grad_norm": 5.81369161605835,
      "learning_rate": 2.9943847656250002e-05,
      "loss": 2.2275,
      "step": 4930
    },
    {
      "epoch": 1.2060546875,
      "grad_norm": 3.402061700820923,
      "learning_rate": 2.9903157552083333e-05,
      "loss": 2.1381,
      "step": 4940
    },
    {
      "epoch": 1.20849609375,
      "grad_norm": 7.07023811340332,
      "learning_rate": 2.986246744791667e-05,
      "loss": 1.9652,
      "step": 4950
    },
    {
      "epoch": 1.2109375,
      "grad_norm": 4.782760143280029,
      "learning_rate": 2.9821777343750002e-05,
      "loss": 2.1481,
      "step": 4960
    },
    {
      "epoch": 1.21337890625,
      "grad_norm": 5.027375221252441,
      "learning_rate": 2.9781087239583333e-05,
      "loss": 1.9773,
      "step": 4970
    },
    {
      "epoch": 1.2158203125,
      "grad_norm": 3.5624725818634033,
      "learning_rate": 2.974039713541667e-05,
      "loss": 1.9961,
      "step": 4980
    },
    {
      "epoch": 1.21826171875,
      "grad_norm": 4.827165603637695,
      "learning_rate": 2.9699707031250002e-05,
      "loss": 2.0825,
      "step": 4990
    },
    {
      "epoch": 1.220703125,
      "grad_norm": 5.752572059631348,
      "learning_rate": 2.9659016927083333e-05,
      "loss": 2.2297,
      "step": 5000
    },
    {
      "epoch": 1.22314453125,
      "grad_norm": 4.150296211242676,
      "learning_rate": 2.961832682291667e-05,
      "loss": 2.3668,
      "step": 5010
    },
    {
      "epoch": 1.2255859375,
      "grad_norm": 6.6056389808654785,
      "learning_rate": 2.9577636718750002e-05,
      "loss": 1.9898,
      "step": 5020
    },
    {
      "epoch": 1.22802734375,
      "grad_norm": 5.059885501861572,
      "learning_rate": 2.9536946614583333e-05,
      "loss": 2.0422,
      "step": 5030
    },
    {
      "epoch": 1.23046875,
      "grad_norm": 3.745389461517334,
      "learning_rate": 2.949625651041667e-05,
      "loss": 2.1068,
      "step": 5040
    },
    {
      "epoch": 1.23291015625,
      "grad_norm": 4.734431266784668,
      "learning_rate": 2.9455566406250002e-05,
      "loss": 2.06,
      "step": 5050
    },
    {
      "epoch": 1.2353515625,
      "grad_norm": 5.392727375030518,
      "learning_rate": 2.9414876302083333e-05,
      "loss": 2.1207,
      "step": 5060
    },
    {
      "epoch": 1.23779296875,
      "grad_norm": 5.499725818634033,
      "learning_rate": 2.937418619791667e-05,
      "loss": 1.9637,
      "step": 5070
    },
    {
      "epoch": 1.240234375,
      "grad_norm": 6.262239456176758,
      "learning_rate": 2.933349609375e-05,
      "loss": 2.0049,
      "step": 5080
    },
    {
      "epoch": 1.24267578125,
      "grad_norm": 6.139383792877197,
      "learning_rate": 2.9292805989583333e-05,
      "loss": 2.0732,
      "step": 5090
    },
    {
      "epoch": 1.2451171875,
      "grad_norm": 4.186737060546875,
      "learning_rate": 2.925211588541667e-05,
      "loss": 2.1126,
      "step": 5100
    },
    {
      "epoch": 1.24755859375,
      "grad_norm": 4.813169002532959,
      "learning_rate": 2.921142578125e-05,
      "loss": 2.083,
      "step": 5110
    },
    {
      "epoch": 1.25,
      "grad_norm": 4.431726455688477,
      "learning_rate": 2.9170735677083333e-05,
      "loss": 2.0467,
      "step": 5120
    },
    {
      "epoch": 1.25244140625,
      "grad_norm": 5.749391078948975,
      "learning_rate": 2.913004557291667e-05,
      "loss": 2.133,
      "step": 5130
    },
    {
      "epoch": 1.2548828125,
      "grad_norm": 5.576328754425049,
      "learning_rate": 2.908935546875e-05,
      "loss": 2.2313,
      "step": 5140
    },
    {
      "epoch": 1.25732421875,
      "grad_norm": 3.933978319168091,
      "learning_rate": 2.9048665364583332e-05,
      "loss": 2.0298,
      "step": 5150
    },
    {
      "epoch": 1.259765625,
      "grad_norm": 4.375457286834717,
      "learning_rate": 2.900797526041667e-05,
      "loss": 2.1003,
      "step": 5160
    },
    {
      "epoch": 1.26220703125,
      "grad_norm": 6.261704921722412,
      "learning_rate": 2.896728515625e-05,
      "loss": 2.1468,
      "step": 5170
    },
    {
      "epoch": 1.2646484375,
      "grad_norm": 5.7412190437316895,
      "learning_rate": 2.8926595052083332e-05,
      "loss": 2.0544,
      "step": 5180
    },
    {
      "epoch": 1.26708984375,
      "grad_norm": 4.002510070800781,
      "learning_rate": 2.888590494791667e-05,
      "loss": 2.2214,
      "step": 5190
    },
    {
      "epoch": 1.26953125,
      "grad_norm": 7.554153919219971,
      "learning_rate": 2.884521484375e-05,
      "loss": 2.1304,
      "step": 5200
    },
    {
      "epoch": 1.27197265625,
      "grad_norm": 5.7513298988342285,
      "learning_rate": 2.8804524739583332e-05,
      "loss": 2.1135,
      "step": 5210
    },
    {
      "epoch": 1.2744140625,
      "grad_norm": 4.686445713043213,
      "learning_rate": 2.876383463541667e-05,
      "loss": 1.8936,
      "step": 5220
    },
    {
      "epoch": 1.27685546875,
      "grad_norm": 6.680769920349121,
      "learning_rate": 2.872314453125e-05,
      "loss": 2.2063,
      "step": 5230
    },
    {
      "epoch": 1.279296875,
      "grad_norm": 6.977116584777832,
      "learning_rate": 2.8682454427083332e-05,
      "loss": 2.0427,
      "step": 5240
    },
    {
      "epoch": 1.28173828125,
      "grad_norm": 4.550504207611084,
      "learning_rate": 2.864176432291667e-05,
      "loss": 2.0966,
      "step": 5250
    },
    {
      "epoch": 1.2841796875,
      "grad_norm": 4.2042155265808105,
      "learning_rate": 2.860107421875e-05,
      "loss": 1.9869,
      "step": 5260
    },
    {
      "epoch": 1.28662109375,
      "grad_norm": 3.785029649734497,
      "learning_rate": 2.8560384114583332e-05,
      "loss": 2.002,
      "step": 5270
    },
    {
      "epoch": 1.2890625,
      "grad_norm": 4.798794269561768,
      "learning_rate": 2.851969401041667e-05,
      "loss": 2.1333,
      "step": 5280
    },
    {
      "epoch": 1.29150390625,
      "grad_norm": 6.019290447235107,
      "learning_rate": 2.847900390625e-05,
      "loss": 2.0591,
      "step": 5290
    },
    {
      "epoch": 1.2939453125,
      "grad_norm": 5.257565975189209,
      "learning_rate": 2.8438313802083332e-05,
      "loss": 2.0863,
      "step": 5300
    },
    {
      "epoch": 1.29638671875,
      "grad_norm": 5.547426700592041,
      "learning_rate": 2.839762369791667e-05,
      "loss": 2.1471,
      "step": 5310
    },
    {
      "epoch": 1.298828125,
      "grad_norm": 8.0253267288208,
      "learning_rate": 2.835693359375e-05,
      "loss": 1.9645,
      "step": 5320
    },
    {
      "epoch": 1.30126953125,
      "grad_norm": 6.8853936195373535,
      "learning_rate": 2.8316243489583332e-05,
      "loss": 2.239,
      "step": 5330
    },
    {
      "epoch": 1.3037109375,
      "grad_norm": 3.8118064403533936,
      "learning_rate": 2.827555338541667e-05,
      "loss": 2.0292,
      "step": 5340
    },
    {
      "epoch": 1.30615234375,
      "grad_norm": 3.7697675228118896,
      "learning_rate": 2.823486328125e-05,
      "loss": 1.9954,
      "step": 5350
    },
    {
      "epoch": 1.30859375,
      "grad_norm": 3.999021530151367,
      "learning_rate": 2.8194173177083332e-05,
      "loss": 2.0324,
      "step": 5360
    },
    {
      "epoch": 1.31103515625,
      "grad_norm": 4.759002208709717,
      "learning_rate": 2.815348307291667e-05,
      "loss": 1.9484,
      "step": 5370
    },
    {
      "epoch": 1.3134765625,
      "grad_norm": 5.547299385070801,
      "learning_rate": 2.811279296875e-05,
      "loss": 2.0604,
      "step": 5380
    },
    {
      "epoch": 1.31591796875,
      "grad_norm": 5.628151893615723,
      "learning_rate": 2.807210286458333e-05,
      "loss": 2.0113,
      "step": 5390
    },
    {
      "epoch": 1.318359375,
      "grad_norm": 4.5698933601379395,
      "learning_rate": 2.803141276041667e-05,
      "loss": 2.0267,
      "step": 5400
    },
    {
      "epoch": 1.32080078125,
      "grad_norm": 4.215269088745117,
      "learning_rate": 2.799072265625e-05,
      "loss": 1.9695,
      "step": 5410
    },
    {
      "epoch": 1.3232421875,
      "grad_norm": 5.9073638916015625,
      "learning_rate": 2.795003255208333e-05,
      "loss": 2.1165,
      "step": 5420
    },
    {
      "epoch": 1.32568359375,
      "grad_norm": 3.8627820014953613,
      "learning_rate": 2.790934244791667e-05,
      "loss": 2.0384,
      "step": 5430
    },
    {
      "epoch": 1.328125,
      "grad_norm": 5.285606384277344,
      "learning_rate": 2.786865234375e-05,
      "loss": 2.011,
      "step": 5440
    },
    {
      "epoch": 1.33056640625,
      "grad_norm": 3.998805284500122,
      "learning_rate": 2.782796223958333e-05,
      "loss": 1.9805,
      "step": 5450
    },
    {
      "epoch": 1.3330078125,
      "grad_norm": 7.273696422576904,
      "learning_rate": 2.778727213541667e-05,
      "loss": 2.084,
      "step": 5460
    },
    {
      "epoch": 1.33544921875,
      "grad_norm": 4.264926433563232,
      "learning_rate": 2.774658203125e-05,
      "loss": 2.1123,
      "step": 5470
    },
    {
      "epoch": 1.337890625,
      "grad_norm": 6.154077529907227,
      "learning_rate": 2.770589192708333e-05,
      "loss": 2.0427,
      "step": 5480
    },
    {
      "epoch": 1.34033203125,
      "grad_norm": 3.7404496669769287,
      "learning_rate": 2.766520182291667e-05,
      "loss": 2.0473,
      "step": 5490
    },
    {
      "epoch": 1.3427734375,
      "grad_norm": 3.7924163341522217,
      "learning_rate": 2.762451171875e-05,
      "loss": 2.2219,
      "step": 5500
    },
    {
      "epoch": 1.34521484375,
      "grad_norm": 4.194286346435547,
      "learning_rate": 2.758382161458333e-05,
      "loss": 2.1065,
      "step": 5510
    },
    {
      "epoch": 1.34765625,
      "grad_norm": 4.972426414489746,
      "learning_rate": 2.754313151041667e-05,
      "loss": 2.0138,
      "step": 5520
    },
    {
      "epoch": 1.35009765625,
      "grad_norm": 3.509071111679077,
      "learning_rate": 2.750244140625e-05,
      "loss": 1.9828,
      "step": 5530
    },
    {
      "epoch": 1.3525390625,
      "grad_norm": 5.297125339508057,
      "learning_rate": 2.7461751302083334e-05,
      "loss": 2.0368,
      "step": 5540
    },
    {
      "epoch": 1.35498046875,
      "grad_norm": 3.990067720413208,
      "learning_rate": 2.742106119791667e-05,
      "loss": 2.0236,
      "step": 5550
    },
    {
      "epoch": 1.357421875,
      "grad_norm": 5.123834133148193,
      "learning_rate": 2.738037109375e-05,
      "loss": 2.0446,
      "step": 5560
    },
    {
      "epoch": 1.35986328125,
      "grad_norm": 7.280013561248779,
      "learning_rate": 2.7339680989583334e-05,
      "loss": 2.1255,
      "step": 5570
    },
    {
      "epoch": 1.3623046875,
      "grad_norm": 4.699408531188965,
      "learning_rate": 2.729899088541667e-05,
      "loss": 2.1307,
      "step": 5580
    },
    {
      "epoch": 1.36474609375,
      "grad_norm": 4.807000637054443,
      "learning_rate": 2.725830078125e-05,
      "loss": 2.154,
      "step": 5590
    },
    {
      "epoch": 1.3671875,
      "grad_norm": 4.443604946136475,
      "learning_rate": 2.7217610677083334e-05,
      "loss": 2.1288,
      "step": 5600
    },
    {
      "epoch": 1.36962890625,
      "grad_norm": 5.395087718963623,
      "learning_rate": 2.717692057291667e-05,
      "loss": 1.9467,
      "step": 5610
    },
    {
      "epoch": 1.3720703125,
      "grad_norm": 4.772629261016846,
      "learning_rate": 2.713623046875e-05,
      "loss": 1.9537,
      "step": 5620
    },
    {
      "epoch": 1.37451171875,
      "grad_norm": 4.6966729164123535,
      "learning_rate": 2.7095540364583334e-05,
      "loss": 2.1353,
      "step": 5630
    },
    {
      "epoch": 1.376953125,
      "grad_norm": 5.378002643585205,
      "learning_rate": 2.705485026041667e-05,
      "loss": 2.0186,
      "step": 5640
    },
    {
      "epoch": 1.37939453125,
      "grad_norm": 5.335057735443115,
      "learning_rate": 2.7014160156250003e-05,
      "loss": 2.1251,
      "step": 5650
    },
    {
      "epoch": 1.3818359375,
      "grad_norm": 5.249743461608887,
      "learning_rate": 2.6973470052083334e-05,
      "loss": 2.1933,
      "step": 5660
    },
    {
      "epoch": 1.38427734375,
      "grad_norm": 7.3191752433776855,
      "learning_rate": 2.693277994791667e-05,
      "loss": 2.1367,
      "step": 5670
    },
    {
      "epoch": 1.38671875,
      "grad_norm": 5.954715251922607,
      "learning_rate": 2.6892089843750003e-05,
      "loss": 1.918,
      "step": 5680
    },
    {
      "epoch": 1.38916015625,
      "grad_norm": 4.4220685958862305,
      "learning_rate": 2.6851399739583334e-05,
      "loss": 2.2092,
      "step": 5690
    },
    {
      "epoch": 1.3916015625,
      "grad_norm": 5.015226364135742,
      "learning_rate": 2.681070963541667e-05,
      "loss": 2.11,
      "step": 5700
    },
    {
      "epoch": 1.39404296875,
      "grad_norm": 3.8985023498535156,
      "learning_rate": 2.6770019531250003e-05,
      "loss": 2.0744,
      "step": 5710
    },
    {
      "epoch": 1.396484375,
      "grad_norm": 5.771401882171631,
      "learning_rate": 2.6729329427083334e-05,
      "loss": 2.1433,
      "step": 5720
    },
    {
      "epoch": 1.39892578125,
      "grad_norm": 4.740551948547363,
      "learning_rate": 2.6688639322916668e-05,
      "loss": 2.3749,
      "step": 5730
    },
    {
      "epoch": 1.4013671875,
      "grad_norm": 4.576526641845703,
      "learning_rate": 2.6647949218750003e-05,
      "loss": 1.932,
      "step": 5740
    },
    {
      "epoch": 1.40380859375,
      "grad_norm": 6.063554763793945,
      "learning_rate": 2.6607259114583334e-05,
      "loss": 2.0852,
      "step": 5750
    },
    {
      "epoch": 1.40625,
      "grad_norm": 5.931682586669922,
      "learning_rate": 2.6566569010416668e-05,
      "loss": 2.2258,
      "step": 5760
    },
    {
      "epoch": 1.40869140625,
      "grad_norm": 5.178934574127197,
      "learning_rate": 2.6525878906250003e-05,
      "loss": 2.0972,
      "step": 5770
    },
    {
      "epoch": 1.4111328125,
      "grad_norm": 3.63147234916687,
      "learning_rate": 2.6485188802083334e-05,
      "loss": 1.9944,
      "step": 5780
    },
    {
      "epoch": 1.41357421875,
      "grad_norm": 5.90288782119751,
      "learning_rate": 2.644449869791667e-05,
      "loss": 1.9295,
      "step": 5790
    },
    {
      "epoch": 1.416015625,
      "grad_norm": 3.407419443130493,
      "learning_rate": 2.6403808593750002e-05,
      "loss": 1.9961,
      "step": 5800
    },
    {
      "epoch": 1.41845703125,
      "grad_norm": 6.396119594573975,
      "learning_rate": 2.6363118489583333e-05,
      "loss": 2.0026,
      "step": 5810
    },
    {
      "epoch": 1.4208984375,
      "grad_norm": 3.8437793254852295,
      "learning_rate": 2.632242838541667e-05,
      "loss": 1.956,
      "step": 5820
    },
    {
      "epoch": 1.42333984375,
      "grad_norm": 3.6740822792053223,
      "learning_rate": 2.6281738281250002e-05,
      "loss": 2.2129,
      "step": 5830
    },
    {
      "epoch": 1.42578125,
      "grad_norm": 4.380774021148682,
      "learning_rate": 2.6241048177083333e-05,
      "loss": 2.0292,
      "step": 5840
    },
    {
      "epoch": 1.42822265625,
      "grad_norm": 4.844695568084717,
      "learning_rate": 2.620035807291667e-05,
      "loss": 1.9811,
      "step": 5850
    },
    {
      "epoch": 1.4306640625,
      "grad_norm": 5.6466827392578125,
      "learning_rate": 2.6159667968750002e-05,
      "loss": 1.8797,
      "step": 5860
    },
    {
      "epoch": 1.43310546875,
      "grad_norm": 6.075017929077148,
      "learning_rate": 2.6118977864583333e-05,
      "loss": 2.0116,
      "step": 5870
    },
    {
      "epoch": 1.435546875,
      "grad_norm": 7.116528034210205,
      "learning_rate": 2.607828776041667e-05,
      "loss": 2.004,
      "step": 5880
    },
    {
      "epoch": 1.43798828125,
      "grad_norm": 4.426695823669434,
      "learning_rate": 2.6037597656250002e-05,
      "loss": 2.0223,
      "step": 5890
    },
    {
      "epoch": 1.4404296875,
      "grad_norm": 5.342060565948486,
      "learning_rate": 2.5996907552083333e-05,
      "loss": 1.9424,
      "step": 5900
    },
    {
      "epoch": 1.44287109375,
      "grad_norm": 4.665234565734863,
      "learning_rate": 2.595621744791667e-05,
      "loss": 2.0578,
      "step": 5910
    },
    {
      "epoch": 1.4453125,
      "grad_norm": 5.816842555999756,
      "learning_rate": 2.5915527343750002e-05,
      "loss": 2.0973,
      "step": 5920
    },
    {
      "epoch": 1.44775390625,
      "grad_norm": 3.8950464725494385,
      "learning_rate": 2.5874837239583333e-05,
      "loss": 1.954,
      "step": 5930
    },
    {
      "epoch": 1.4501953125,
      "grad_norm": 5.413497447967529,
      "learning_rate": 2.583414713541667e-05,
      "loss": 2.0854,
      "step": 5940
    },
    {
      "epoch": 1.45263671875,
      "grad_norm": 5.909020900726318,
      "learning_rate": 2.5793457031250002e-05,
      "loss": 2.1009,
      "step": 5950
    },
    {
      "epoch": 1.455078125,
      "grad_norm": 6.177698135375977,
      "learning_rate": 2.5752766927083333e-05,
      "loss": 1.8378,
      "step": 5960
    },
    {
      "epoch": 1.45751953125,
      "grad_norm": 6.349514961242676,
      "learning_rate": 2.571207682291667e-05,
      "loss": 2.0489,
      "step": 5970
    },
    {
      "epoch": 1.4599609375,
      "grad_norm": 5.165702819824219,
      "learning_rate": 2.5671386718750002e-05,
      "loss": 2.0776,
      "step": 5980
    },
    {
      "epoch": 1.46240234375,
      "grad_norm": 4.44524621963501,
      "learning_rate": 2.5630696614583333e-05,
      "loss": 2.0337,
      "step": 5990
    },
    {
      "epoch": 1.46484375,
      "grad_norm": 4.279562950134277,
      "learning_rate": 2.559000651041667e-05,
      "loss": 1.9183,
      "step": 6000
    },
    {
      "epoch": 1.46728515625,
      "grad_norm": 4.340635299682617,
      "learning_rate": 2.554931640625e-05,
      "loss": 2.1018,
      "step": 6010
    },
    {
      "epoch": 1.4697265625,
      "grad_norm": 6.384187698364258,
      "learning_rate": 2.5508626302083333e-05,
      "loss": 2.0355,
      "step": 6020
    },
    {
      "epoch": 1.47216796875,
      "grad_norm": 4.636345863342285,
      "learning_rate": 2.546793619791667e-05,
      "loss": 1.8104,
      "step": 6030
    },
    {
      "epoch": 1.474609375,
      "grad_norm": 3.501336097717285,
      "learning_rate": 2.542724609375e-05,
      "loss": 1.9832,
      "step": 6040
    },
    {
      "epoch": 1.47705078125,
      "grad_norm": 4.860774517059326,
      "learning_rate": 2.5386555989583333e-05,
      "loss": 2.0069,
      "step": 6050
    },
    {
      "epoch": 1.4794921875,
      "grad_norm": 3.6316118240356445,
      "learning_rate": 2.534586588541667e-05,
      "loss": 2.0562,
      "step": 6060
    },
    {
      "epoch": 1.48193359375,
      "grad_norm": 5.89840841293335,
      "learning_rate": 2.530517578125e-05,
      "loss": 2.1087,
      "step": 6070
    },
    {
      "epoch": 1.484375,
      "grad_norm": 6.213653087615967,
      "learning_rate": 2.5264485677083333e-05,
      "loss": 1.9774,
      "step": 6080
    },
    {
      "epoch": 1.48681640625,
      "grad_norm": 7.480350494384766,
      "learning_rate": 2.522379557291667e-05,
      "loss": 1.8084,
      "step": 6090
    },
    {
      "epoch": 1.4892578125,
      "grad_norm": 5.292077541351318,
      "learning_rate": 2.518310546875e-05,
      "loss": 1.9843,
      "step": 6100
    },
    {
      "epoch": 1.49169921875,
      "grad_norm": 5.6160783767700195,
      "learning_rate": 2.5142415364583332e-05,
      "loss": 1.9764,
      "step": 6110
    },
    {
      "epoch": 1.494140625,
      "grad_norm": 6.562152862548828,
      "learning_rate": 2.510172526041667e-05,
      "loss": 2.0289,
      "step": 6120
    },
    {
      "epoch": 1.49658203125,
      "grad_norm": 11.920845031738281,
      "learning_rate": 2.506103515625e-05,
      "loss": 2.3079,
      "step": 6130
    },
    {
      "epoch": 1.4990234375,
      "grad_norm": 4.244705677032471,
      "learning_rate": 2.5020345052083332e-05,
      "loss": 1.9365,
      "step": 6140
    },
    {
      "epoch": 1.50146484375,
      "grad_norm": 3.975231170654297,
      "learning_rate": 2.4979654947916667e-05,
      "loss": 2.205,
      "step": 6150
    },
    {
      "epoch": 1.50390625,
      "grad_norm": 5.2123517990112305,
      "learning_rate": 2.493896484375e-05,
      "loss": 1.9226,
      "step": 6160
    },
    {
      "epoch": 1.50634765625,
      "grad_norm": 7.083162784576416,
      "learning_rate": 2.4898274739583336e-05,
      "loss": 2.0822,
      "step": 6170
    },
    {
      "epoch": 1.5087890625,
      "grad_norm": 4.352183818817139,
      "learning_rate": 2.4857584635416667e-05,
      "loss": 2.0616,
      "step": 6180
    },
    {
      "epoch": 1.51123046875,
      "grad_norm": 3.232510805130005,
      "learning_rate": 2.481689453125e-05,
      "loss": 2.1186,
      "step": 6190
    },
    {
      "epoch": 1.513671875,
      "grad_norm": 4.2204108238220215,
      "learning_rate": 2.4776204427083335e-05,
      "loss": 2.075,
      "step": 6200
    },
    {
      "epoch": 1.51611328125,
      "grad_norm": 7.324868679046631,
      "learning_rate": 2.4735514322916667e-05,
      "loss": 2.0216,
      "step": 6210
    },
    {
      "epoch": 1.5185546875,
      "grad_norm": 4.946828365325928,
      "learning_rate": 2.469482421875e-05,
      "loss": 1.9594,
      "step": 6220
    },
    {
      "epoch": 1.52099609375,
      "grad_norm": 4.493707180023193,
      "learning_rate": 2.4654134114583335e-05,
      "loss": 2.1283,
      "step": 6230
    },
    {
      "epoch": 1.5234375,
      "grad_norm": 3.243062973022461,
      "learning_rate": 2.4613444010416666e-05,
      "loss": 2.1597,
      "step": 6240
    },
    {
      "epoch": 1.52587890625,
      "grad_norm": 4.279540061950684,
      "learning_rate": 2.457275390625e-05,
      "loss": 1.9647,
      "step": 6250
    },
    {
      "epoch": 1.5283203125,
      "grad_norm": 4.882023811340332,
      "learning_rate": 2.4532063802083335e-05,
      "loss": 1.9611,
      "step": 6260
    },
    {
      "epoch": 1.53076171875,
      "grad_norm": 4.37914514541626,
      "learning_rate": 2.4491373697916666e-05,
      "loss": 1.9815,
      "step": 6270
    },
    {
      "epoch": 1.533203125,
      "grad_norm": 5.234164237976074,
      "learning_rate": 2.445068359375e-05,
      "loss": 2.1409,
      "step": 6280
    },
    {
      "epoch": 1.53564453125,
      "grad_norm": 5.068321228027344,
      "learning_rate": 2.4409993489583335e-05,
      "loss": 2.1859,
      "step": 6290
    },
    {
      "epoch": 1.5380859375,
      "grad_norm": 4.890941143035889,
      "learning_rate": 2.4369303385416666e-05,
      "loss": 2.1246,
      "step": 6300
    },
    {
      "epoch": 1.54052734375,
      "grad_norm": 5.913109302520752,
      "learning_rate": 2.432861328125e-05,
      "loss": 2.2851,
      "step": 6310
    },
    {
      "epoch": 1.54296875,
      "grad_norm": 5.249934196472168,
      "learning_rate": 2.4287923177083335e-05,
      "loss": 2.0476,
      "step": 6320
    },
    {
      "epoch": 1.54541015625,
      "grad_norm": 6.651054382324219,
      "learning_rate": 2.4247233072916666e-05,
      "loss": 1.9234,
      "step": 6330
    },
    {
      "epoch": 1.5478515625,
      "grad_norm": 5.438218593597412,
      "learning_rate": 2.420654296875e-05,
      "loss": 2.0939,
      "step": 6340
    },
    {
      "epoch": 1.55029296875,
      "grad_norm": 8.719407081604004,
      "learning_rate": 2.4165852864583335e-05,
      "loss": 2.1386,
      "step": 6350
    },
    {
      "epoch": 1.552734375,
      "grad_norm": 7.468752384185791,
      "learning_rate": 2.4125162760416666e-05,
      "loss": 1.9285,
      "step": 6360
    },
    {
      "epoch": 1.55517578125,
      "grad_norm": 4.385725975036621,
      "learning_rate": 2.408447265625e-05,
      "loss": 1.8226,
      "step": 6370
    },
    {
      "epoch": 1.5576171875,
      "grad_norm": 7.208509922027588,
      "learning_rate": 2.4043782552083335e-05,
      "loss": 2.0546,
      "step": 6380
    },
    {
      "epoch": 1.56005859375,
      "grad_norm": 5.607935905456543,
      "learning_rate": 2.4003092447916666e-05,
      "loss": 2.1766,
      "step": 6390
    },
    {
      "epoch": 1.5625,
      "grad_norm": 8.616363525390625,
      "learning_rate": 2.396240234375e-05,
      "loss": 2.0154,
      "step": 6400
    },
    {
      "epoch": 1.56494140625,
      "grad_norm": 6.456578254699707,
      "learning_rate": 2.3921712239583335e-05,
      "loss": 2.0219,
      "step": 6410
    },
    {
      "epoch": 1.5673828125,
      "grad_norm": 9.545162200927734,
      "learning_rate": 2.3881022135416666e-05,
      "loss": 2.0403,
      "step": 6420
    },
    {
      "epoch": 1.56982421875,
      "grad_norm": 5.6471662521362305,
      "learning_rate": 2.384033203125e-05,
      "loss": 2.2262,
      "step": 6430
    },
    {
      "epoch": 1.572265625,
      "grad_norm": 4.624783992767334,
      "learning_rate": 2.3799641927083335e-05,
      "loss": 1.9319,
      "step": 6440
    },
    {
      "epoch": 1.57470703125,
      "grad_norm": 6.210513114929199,
      "learning_rate": 2.3758951822916666e-05,
      "loss": 2.3099,
      "step": 6450
    },
    {
      "epoch": 1.5771484375,
      "grad_norm": 4.346573829650879,
      "learning_rate": 2.371826171875e-05,
      "loss": 2.1481,
      "step": 6460
    },
    {
      "epoch": 1.57958984375,
      "grad_norm": 5.4894328117370605,
      "learning_rate": 2.3677571614583334e-05,
      "loss": 2.0256,
      "step": 6470
    },
    {
      "epoch": 1.58203125,
      "grad_norm": 4.550121784210205,
      "learning_rate": 2.3636881510416666e-05,
      "loss": 2.2153,
      "step": 6480
    },
    {
      "epoch": 1.58447265625,
      "grad_norm": 3.7926218509674072,
      "learning_rate": 2.359619140625e-05,
      "loss": 1.953,
      "step": 6490
    },
    {
      "epoch": 1.5869140625,
      "grad_norm": 6.832355976104736,
      "learning_rate": 2.3555501302083334e-05,
      "loss": 2.1864,
      "step": 6500
    },
    {
      "epoch": 1.58935546875,
      "grad_norm": 4.299874782562256,
      "learning_rate": 2.3514811197916665e-05,
      "loss": 1.9066,
      "step": 6510
    },
    {
      "epoch": 1.591796875,
      "grad_norm": 5.532857418060303,
      "learning_rate": 2.347412109375e-05,
      "loss": 1.9765,
      "step": 6520
    },
    {
      "epoch": 1.59423828125,
      "grad_norm": 6.225566387176514,
      "learning_rate": 2.3433430989583334e-05,
      "loss": 2.184,
      "step": 6530
    },
    {
      "epoch": 1.5966796875,
      "grad_norm": 4.794334411621094,
      "learning_rate": 2.3392740885416665e-05,
      "loss": 1.8969,
      "step": 6540
    },
    {
      "epoch": 1.59912109375,
      "grad_norm": 6.187029838562012,
      "learning_rate": 2.335205078125e-05,
      "loss": 2.0911,
      "step": 6550
    },
    {
      "epoch": 1.6015625,
      "grad_norm": 3.71821665763855,
      "learning_rate": 2.3311360677083334e-05,
      "loss": 2.0746,
      "step": 6560
    },
    {
      "epoch": 1.60400390625,
      "grad_norm": 3.508138418197632,
      "learning_rate": 2.3270670572916665e-05,
      "loss": 2.0799,
      "step": 6570
    },
    {
      "epoch": 1.6064453125,
      "grad_norm": 6.711729049682617,
      "learning_rate": 2.322998046875e-05,
      "loss": 2.242,
      "step": 6580
    },
    {
      "epoch": 1.60888671875,
      "grad_norm": 4.377861499786377,
      "learning_rate": 2.3189290364583334e-05,
      "loss": 2.0635,
      "step": 6590
    },
    {
      "epoch": 1.611328125,
      "grad_norm": 3.492330312728882,
      "learning_rate": 2.314860026041667e-05,
      "loss": 1.8729,
      "step": 6600
    },
    {
      "epoch": 1.61376953125,
      "grad_norm": 5.477457046508789,
      "learning_rate": 2.310791015625e-05,
      "loss": 2.2012,
      "step": 6610
    },
    {
      "epoch": 1.6162109375,
      "grad_norm": 3.3863203525543213,
      "learning_rate": 2.3067220052083334e-05,
      "loss": 1.8888,
      "step": 6620
    },
    {
      "epoch": 1.61865234375,
      "grad_norm": 5.54107666015625,
      "learning_rate": 2.302652994791667e-05,
      "loss": 1.9634,
      "step": 6630
    },
    {
      "epoch": 1.62109375,
      "grad_norm": 6.285275459289551,
      "learning_rate": 2.298583984375e-05,
      "loss": 1.8614,
      "step": 6640
    },
    {
      "epoch": 1.62353515625,
      "grad_norm": 6.405539035797119,
      "learning_rate": 2.2945149739583334e-05,
      "loss": 2.1138,
      "step": 6650
    },
    {
      "epoch": 1.6259765625,
      "grad_norm": 3.4754080772399902,
      "learning_rate": 2.2904459635416668e-05,
      "loss": 2.0214,
      "step": 6660
    },
    {
      "epoch": 1.62841796875,
      "grad_norm": 6.835118770599365,
      "learning_rate": 2.2863769531250003e-05,
      "loss": 2.0049,
      "step": 6670
    },
    {
      "epoch": 1.630859375,
      "grad_norm": 6.100186347961426,
      "learning_rate": 2.2823079427083334e-05,
      "loss": 2.0093,
      "step": 6680
    },
    {
      "epoch": 1.63330078125,
      "grad_norm": 6.657668113708496,
      "learning_rate": 2.2782389322916668e-05,
      "loss": 1.9972,
      "step": 6690
    },
    {
      "epoch": 1.6357421875,
      "grad_norm": 4.6630539894104,
      "learning_rate": 2.2741699218750003e-05,
      "loss": 2.0308,
      "step": 6700
    },
    {
      "epoch": 1.63818359375,
      "grad_norm": 7.423729419708252,
      "learning_rate": 2.2701009114583337e-05,
      "loss": 1.9407,
      "step": 6710
    },
    {
      "epoch": 1.640625,
      "grad_norm": 7.693170547485352,
      "learning_rate": 2.2660319010416668e-05,
      "loss": 2.1874,
      "step": 6720
    },
    {
      "epoch": 1.64306640625,
      "grad_norm": 6.660158157348633,
      "learning_rate": 2.2619628906250002e-05,
      "loss": 1.942,
      "step": 6730
    },
    {
      "epoch": 1.6455078125,
      "grad_norm": 7.053839683532715,
      "learning_rate": 2.2578938802083337e-05,
      "loss": 1.9529,
      "step": 6740
    },
    {
      "epoch": 1.64794921875,
      "grad_norm": 6.092179298400879,
      "learning_rate": 2.2538248697916668e-05,
      "loss": 1.9843,
      "step": 6750
    },
    {
      "epoch": 1.650390625,
      "grad_norm": 5.714369297027588,
      "learning_rate": 2.2497558593750002e-05,
      "loss": 1.9425,
      "step": 6760
    },
    {
      "epoch": 1.65283203125,
      "grad_norm": 6.32726526260376,
      "learning_rate": 2.2456868489583337e-05,
      "loss": 2.0364,
      "step": 6770
    },
    {
      "epoch": 1.6552734375,
      "grad_norm": 4.591183662414551,
      "learning_rate": 2.2416178385416668e-05,
      "loss": 2.0731,
      "step": 6780
    },
    {
      "epoch": 1.65771484375,
      "grad_norm": 3.735832452774048,
      "learning_rate": 2.2375488281250002e-05,
      "loss": 2.146,
      "step": 6790
    },
    {
      "epoch": 1.66015625,
      "grad_norm": 4.225723743438721,
      "learning_rate": 2.2334798177083337e-05,
      "loss": 2.0781,
      "step": 6800
    },
    {
      "epoch": 1.66259765625,
      "grad_norm": 4.395215034484863,
      "learning_rate": 2.2294108072916668e-05,
      "loss": 1.909,
      "step": 6810
    },
    {
      "epoch": 1.6650390625,
      "grad_norm": 4.277565956115723,
      "learning_rate": 2.2253417968750002e-05,
      "loss": 2.0371,
      "step": 6820
    },
    {
      "epoch": 1.66748046875,
      "grad_norm": 3.3667516708374023,
      "learning_rate": 2.2212727864583337e-05,
      "loss": 2.1066,
      "step": 6830
    },
    {
      "epoch": 1.669921875,
      "grad_norm": 4.9070563316345215,
      "learning_rate": 2.2172037760416668e-05,
      "loss": 1.9759,
      "step": 6840
    },
    {
      "epoch": 1.67236328125,
      "grad_norm": 3.6053621768951416,
      "learning_rate": 2.2131347656250002e-05,
      "loss": 1.9061,
      "step": 6850
    },
    {
      "epoch": 1.6748046875,
      "grad_norm": 4.000538349151611,
      "learning_rate": 2.2090657552083336e-05,
      "loss": 1.9848,
      "step": 6860
    },
    {
      "epoch": 1.67724609375,
      "grad_norm": 6.1365065574646,
      "learning_rate": 2.2049967447916668e-05,
      "loss": 2.116,
      "step": 6870
    },
    {
      "epoch": 1.6796875,
      "grad_norm": 4.2930145263671875,
      "learning_rate": 2.2009277343750002e-05,
      "loss": 2.0959,
      "step": 6880
    },
    {
      "epoch": 1.68212890625,
      "grad_norm": 4.451362133026123,
      "learning_rate": 2.1968587239583336e-05,
      "loss": 2.0569,
      "step": 6890
    },
    {
      "epoch": 1.6845703125,
      "grad_norm": 4.8097968101501465,
      "learning_rate": 2.1927897135416667e-05,
      "loss": 2.0972,
      "step": 6900
    },
    {
      "epoch": 1.68701171875,
      "grad_norm": 4.788939952850342,
      "learning_rate": 2.1887207031250002e-05,
      "loss": 2.003,
      "step": 6910
    },
    {
      "epoch": 1.689453125,
      "grad_norm": 3.936697483062744,
      "learning_rate": 2.1846516927083336e-05,
      "loss": 2.0153,
      "step": 6920
    },
    {
      "epoch": 1.69189453125,
      "grad_norm": 4.795139789581299,
      "learning_rate": 2.1805826822916667e-05,
      "loss": 1.9747,
      "step": 6930
    },
    {
      "epoch": 1.6943359375,
      "grad_norm": 3.6597509384155273,
      "learning_rate": 2.1765136718750002e-05,
      "loss": 2.0566,
      "step": 6940
    },
    {
      "epoch": 1.69677734375,
      "grad_norm": 3.437537908554077,
      "learning_rate": 2.1724446614583336e-05,
      "loss": 1.9054,
      "step": 6950
    },
    {
      "epoch": 1.69921875,
      "grad_norm": 5.588447093963623,
      "learning_rate": 2.1683756510416667e-05,
      "loss": 1.9433,
      "step": 6960
    },
    {
      "epoch": 1.70166015625,
      "grad_norm": 7.653008937835693,
      "learning_rate": 2.164306640625e-05,
      "loss": 2.086,
      "step": 6970
    },
    {
      "epoch": 1.7041015625,
      "grad_norm": 4.151998043060303,
      "learning_rate": 2.1602376302083336e-05,
      "loss": 2.0319,
      "step": 6980
    },
    {
      "epoch": 1.70654296875,
      "grad_norm": 6.492679119110107,
      "learning_rate": 2.1561686197916667e-05,
      "loss": 2.112,
      "step": 6990
    },
    {
      "epoch": 1.708984375,
      "grad_norm": 5.759103775024414,
      "learning_rate": 2.152099609375e-05,
      "loss": 2.0936,
      "step": 7000
    },
    {
      "epoch": 1.71142578125,
      "grad_norm": 5.321379661560059,
      "learning_rate": 2.1480305989583336e-05,
      "loss": 1.9433,
      "step": 7010
    },
    {
      "epoch": 1.7138671875,
      "grad_norm": 4.850794315338135,
      "learning_rate": 2.1439615885416667e-05,
      "loss": 2.0383,
      "step": 7020
    },
    {
      "epoch": 1.71630859375,
      "grad_norm": 6.068665027618408,
      "learning_rate": 2.139892578125e-05,
      "loss": 1.9006,
      "step": 7030
    },
    {
      "epoch": 1.71875,
      "grad_norm": 5.017831325531006,
      "learning_rate": 2.1358235677083336e-05,
      "loss": 1.9804,
      "step": 7040
    },
    {
      "epoch": 1.72119140625,
      "grad_norm": 4.834994316101074,
      "learning_rate": 2.1317545572916667e-05,
      "loss": 1.9008,
      "step": 7050
    },
    {
      "epoch": 1.7236328125,
      "grad_norm": 4.674922466278076,
      "learning_rate": 2.127685546875e-05,
      "loss": 1.9813,
      "step": 7060
    },
    {
      "epoch": 1.72607421875,
      "grad_norm": 3.581533193588257,
      "learning_rate": 2.1236165364583336e-05,
      "loss": 1.9516,
      "step": 7070
    },
    {
      "epoch": 1.728515625,
      "grad_norm": 8.717934608459473,
      "learning_rate": 2.1195475260416667e-05,
      "loss": 2.099,
      "step": 7080
    },
    {
      "epoch": 1.73095703125,
      "grad_norm": 5.582571983337402,
      "learning_rate": 2.115478515625e-05,
      "loss": 2.4539,
      "step": 7090
    },
    {
      "epoch": 1.7333984375,
      "grad_norm": 3.5361833572387695,
      "learning_rate": 2.1114095052083336e-05,
      "loss": 1.897,
      "step": 7100
    },
    {
      "epoch": 1.73583984375,
      "grad_norm": 4.386842250823975,
      "learning_rate": 2.1073404947916667e-05,
      "loss": 2.0975,
      "step": 7110
    },
    {
      "epoch": 1.73828125,
      "grad_norm": 3.533698558807373,
      "learning_rate": 2.103271484375e-05,
      "loss": 1.9078,
      "step": 7120
    },
    {
      "epoch": 1.74072265625,
      "grad_norm": 4.622344017028809,
      "learning_rate": 2.0992024739583335e-05,
      "loss": 1.9,
      "step": 7130
    },
    {
      "epoch": 1.7431640625,
      "grad_norm": 5.208755016326904,
      "learning_rate": 2.0951334635416667e-05,
      "loss": 2.1211,
      "step": 7140
    },
    {
      "epoch": 1.74560546875,
      "grad_norm": 4.834759712219238,
      "learning_rate": 2.091064453125e-05,
      "loss": 1.964,
      "step": 7150
    },
    {
      "epoch": 1.748046875,
      "grad_norm": 7.120843887329102,
      "learning_rate": 2.0869954427083335e-05,
      "loss": 2.0731,
      "step": 7160
    },
    {
      "epoch": 1.75048828125,
      "grad_norm": 5.172531604766846,
      "learning_rate": 2.0829264322916666e-05,
      "loss": 1.9371,
      "step": 7170
    },
    {
      "epoch": 1.7529296875,
      "grad_norm": 5.400550365447998,
      "learning_rate": 2.078857421875e-05,
      "loss": 2.0375,
      "step": 7180
    },
    {
      "epoch": 1.75537109375,
      "grad_norm": 4.350658416748047,
      "learning_rate": 2.0747884114583335e-05,
      "loss": 2.0259,
      "step": 7190
    },
    {
      "epoch": 1.7578125,
      "grad_norm": 3.8227334022521973,
      "learning_rate": 2.0707194010416666e-05,
      "loss": 2.0525,
      "step": 7200
    },
    {
      "epoch": 1.76025390625,
      "grad_norm": 4.846637725830078,
      "learning_rate": 2.066650390625e-05,
      "loss": 2.0416,
      "step": 7210
    },
    {
      "epoch": 1.7626953125,
      "grad_norm": 4.08543586730957,
      "learning_rate": 2.0625813802083335e-05,
      "loss": 2.1136,
      "step": 7220
    },
    {
      "epoch": 1.76513671875,
      "grad_norm": 6.433887481689453,
      "learning_rate": 2.0585123697916666e-05,
      "loss": 1.9212,
      "step": 7230
    },
    {
      "epoch": 1.767578125,
      "grad_norm": 4.731052398681641,
      "learning_rate": 2.054443359375e-05,
      "loss": 2.1607,
      "step": 7240
    },
    {
      "epoch": 1.77001953125,
      "grad_norm": 4.498788356781006,
      "learning_rate": 2.0503743489583335e-05,
      "loss": 1.7693,
      "step": 7250
    },
    {
      "epoch": 1.7724609375,
      "grad_norm": 4.432816505432129,
      "learning_rate": 2.0463053385416666e-05,
      "loss": 1.9461,
      "step": 7260
    },
    {
      "epoch": 1.77490234375,
      "grad_norm": 4.873483657836914,
      "learning_rate": 2.042236328125e-05,
      "loss": 2.1204,
      "step": 7270
    },
    {
      "epoch": 1.77734375,
      "grad_norm": 5.941573143005371,
      "learning_rate": 2.0381673177083335e-05,
      "loss": 1.9635,
      "step": 7280
    },
    {
      "epoch": 1.77978515625,
      "grad_norm": 7.204107761383057,
      "learning_rate": 2.0340983072916666e-05,
      "loss": 1.9954,
      "step": 7290
    },
    {
      "epoch": 1.7822265625,
      "grad_norm": 5.6490888595581055,
      "learning_rate": 2.030029296875e-05,
      "loss": 2.244,
      "step": 7300
    },
    {
      "epoch": 1.78466796875,
      "grad_norm": 6.0761332511901855,
      "learning_rate": 2.0259602864583335e-05,
      "loss": 2.182,
      "step": 7310
    },
    {
      "epoch": 1.787109375,
      "grad_norm": 4.955999374389648,
      "learning_rate": 2.0218912760416666e-05,
      "loss": 2.0175,
      "step": 7320
    },
    {
      "epoch": 1.78955078125,
      "grad_norm": 4.282842636108398,
      "learning_rate": 2.017822265625e-05,
      "loss": 1.9344,
      "step": 7330
    },
    {
      "epoch": 1.7919921875,
      "grad_norm": 4.745975017547607,
      "learning_rate": 2.0137532552083335e-05,
      "loss": 2.0136,
      "step": 7340
    },
    {
      "epoch": 1.79443359375,
      "grad_norm": 4.973791599273682,
      "learning_rate": 2.0096842447916666e-05,
      "loss": 2.0693,
      "step": 7350
    },
    {
      "epoch": 1.796875,
      "grad_norm": 6.259021759033203,
      "learning_rate": 2.005615234375e-05,
      "loss": 1.929,
      "step": 7360
    },
    {
      "epoch": 1.79931640625,
      "grad_norm": 6.209288120269775,
      "learning_rate": 2.0015462239583335e-05,
      "loss": 1.9488,
      "step": 7370
    },
    {
      "epoch": 1.8017578125,
      "grad_norm": 5.468893527984619,
      "learning_rate": 1.9974772135416666e-05,
      "loss": 2.0661,
      "step": 7380
    },
    {
      "epoch": 1.80419921875,
      "grad_norm": 3.019942283630371,
      "learning_rate": 1.993408203125e-05,
      "loss": 2.0416,
      "step": 7390
    },
    {
      "epoch": 1.806640625,
      "grad_norm": 3.8193790912628174,
      "learning_rate": 1.9893391927083335e-05,
      "loss": 1.8706,
      "step": 7400
    },
    {
      "epoch": 1.80908203125,
      "grad_norm": 6.015581130981445,
      "learning_rate": 1.9852701822916666e-05,
      "loss": 2.0769,
      "step": 7410
    },
    {
      "epoch": 1.8115234375,
      "grad_norm": 5.147666931152344,
      "learning_rate": 1.981201171875e-05,
      "loss": 2.3229,
      "step": 7420
    },
    {
      "epoch": 1.81396484375,
      "grad_norm": 5.758225440979004,
      "learning_rate": 1.9771321614583334e-05,
      "loss": 2.01,
      "step": 7430
    },
    {
      "epoch": 1.81640625,
      "grad_norm": 4.209668159484863,
      "learning_rate": 1.9730631510416665e-05,
      "loss": 1.965,
      "step": 7440
    },
    {
      "epoch": 1.81884765625,
      "grad_norm": 8.815540313720703,
      "learning_rate": 1.968994140625e-05,
      "loss": 2.07,
      "step": 7450
    },
    {
      "epoch": 1.8212890625,
      "grad_norm": 5.741090774536133,
      "learning_rate": 1.9649251302083334e-05,
      "loss": 1.876,
      "step": 7460
    },
    {
      "epoch": 1.82373046875,
      "grad_norm": 3.833054780960083,
      "learning_rate": 1.9608561197916665e-05,
      "loss": 2.0196,
      "step": 7470
    },
    {
      "epoch": 1.826171875,
      "grad_norm": 4.379715442657471,
      "learning_rate": 1.956787109375e-05,
      "loss": 1.9975,
      "step": 7480
    },
    {
      "epoch": 1.82861328125,
      "grad_norm": 6.1547932624816895,
      "learning_rate": 1.9527180989583334e-05,
      "loss": 2.0935,
      "step": 7490
    },
    {
      "epoch": 1.8310546875,
      "grad_norm": 5.386590480804443,
      "learning_rate": 1.9486490885416665e-05,
      "loss": 2.218,
      "step": 7500
    },
    {
      "epoch": 1.83349609375,
      "grad_norm": 5.41179895401001,
      "learning_rate": 1.944580078125e-05,
      "loss": 2.1549,
      "step": 7510
    },
    {
      "epoch": 1.8359375,
      "grad_norm": 4.209761142730713,
      "learning_rate": 1.9405110677083334e-05,
      "loss": 1.8306,
      "step": 7520
    },
    {
      "epoch": 1.83837890625,
      "grad_norm": 3.7889883518218994,
      "learning_rate": 1.9364420572916665e-05,
      "loss": 2.0137,
      "step": 7530
    },
    {
      "epoch": 1.8408203125,
      "grad_norm": 4.21850061416626,
      "learning_rate": 1.932373046875e-05,
      "loss": 1.9123,
      "step": 7540
    },
    {
      "epoch": 1.84326171875,
      "grad_norm": 8.434798240661621,
      "learning_rate": 1.9283040364583334e-05,
      "loss": 1.9966,
      "step": 7550
    },
    {
      "epoch": 1.845703125,
      "grad_norm": 6.786678791046143,
      "learning_rate": 1.9242350260416665e-05,
      "loss": 2.1483,
      "step": 7560
    },
    {
      "epoch": 1.84814453125,
      "grad_norm": 5.017823696136475,
      "learning_rate": 1.920166015625e-05,
      "loss": 1.8861,
      "step": 7570
    },
    {
      "epoch": 1.8505859375,
      "grad_norm": 5.729621410369873,
      "learning_rate": 1.9160970052083334e-05,
      "loss": 2.0401,
      "step": 7580
    },
    {
      "epoch": 1.85302734375,
      "grad_norm": 5.611079216003418,
      "learning_rate": 1.9120279947916668e-05,
      "loss": 2.0258,
      "step": 7590
    },
    {
      "epoch": 1.85546875,
      "grad_norm": 5.370690822601318,
      "learning_rate": 1.907958984375e-05,
      "loss": 2.0973,
      "step": 7600
    },
    {
      "epoch": 1.85791015625,
      "grad_norm": 4.15348482131958,
      "learning_rate": 1.9038899739583334e-05,
      "loss": 2.1406,
      "step": 7610
    },
    {
      "epoch": 1.8603515625,
      "grad_norm": 5.412433624267578,
      "learning_rate": 1.8998209635416668e-05,
      "loss": 2.0599,
      "step": 7620
    },
    {
      "epoch": 1.86279296875,
      "grad_norm": 8.080329895019531,
      "learning_rate": 1.895751953125e-05,
      "loss": 1.994,
      "step": 7630
    },
    {
      "epoch": 1.865234375,
      "grad_norm": 6.347524166107178,
      "learning_rate": 1.8916829427083334e-05,
      "loss": 1.9683,
      "step": 7640
    },
    {
      "epoch": 1.86767578125,
      "grad_norm": 7.0244364738464355,
      "learning_rate": 1.8876139322916668e-05,
      "loss": 2.2639,
      "step": 7650
    },
    {
      "epoch": 1.8701171875,
      "grad_norm": 4.996602535247803,
      "learning_rate": 1.8835449218750002e-05,
      "loss": 2.0683,
      "step": 7660
    },
    {
      "epoch": 1.87255859375,
      "grad_norm": 4.640044689178467,
      "learning_rate": 1.8794759114583334e-05,
      "loss": 2.1224,
      "step": 7670
    },
    {
      "epoch": 1.875,
      "grad_norm": 4.6340789794921875,
      "learning_rate": 1.8754069010416668e-05,
      "loss": 2.1411,
      "step": 7680
    },
    {
      "epoch": 1.87744140625,
      "grad_norm": 5.936351299285889,
      "learning_rate": 1.8713378906250002e-05,
      "loss": 2.046,
      "step": 7690
    },
    {
      "epoch": 1.8798828125,
      "grad_norm": 4.794037818908691,
      "learning_rate": 1.8672688802083337e-05,
      "loss": 1.9769,
      "step": 7700
    },
    {
      "epoch": 1.88232421875,
      "grad_norm": 4.006584167480469,
      "learning_rate": 1.8631998697916668e-05,
      "loss": 2.0864,
      "step": 7710
    },
    {
      "epoch": 1.884765625,
      "grad_norm": 5.911752700805664,
      "learning_rate": 1.8591308593750002e-05,
      "loss": 2.1249,
      "step": 7720
    },
    {
      "epoch": 1.88720703125,
      "grad_norm": 5.0694260597229,
      "learning_rate": 1.8550618489583337e-05,
      "loss": 2.1968,
      "step": 7730
    },
    {
      "epoch": 1.8896484375,
      "grad_norm": 4.276596546173096,
      "learning_rate": 1.8509928385416668e-05,
      "loss": 2.0958,
      "step": 7740
    },
    {
      "epoch": 1.89208984375,
      "grad_norm": 3.907407283782959,
      "learning_rate": 1.8469238281250002e-05,
      "loss": 1.9848,
      "step": 7750
    },
    {
      "epoch": 1.89453125,
      "grad_norm": 4.815723896026611,
      "learning_rate": 1.8428548177083337e-05,
      "loss": 2.0177,
      "step": 7760
    },
    {
      "epoch": 1.89697265625,
      "grad_norm": 3.7194786071777344,
      "learning_rate": 1.8387858072916668e-05,
      "loss": 2.1415,
      "step": 7770
    },
    {
      "epoch": 1.8994140625,
      "grad_norm": 5.748584747314453,
      "learning_rate": 1.8347167968750002e-05,
      "loss": 1.9255,
      "step": 7780
    },
    {
      "epoch": 1.90185546875,
      "grad_norm": 8.045658111572266,
      "learning_rate": 1.8306477864583336e-05,
      "loss": 1.9428,
      "step": 7790
    },
    {
      "epoch": 1.904296875,
      "grad_norm": 7.7071003913879395,
      "learning_rate": 1.8265787760416668e-05,
      "loss": 1.942,
      "step": 7800
    },
    {
      "epoch": 1.90673828125,
      "grad_norm": 6.228553295135498,
      "learning_rate": 1.8225097656250002e-05,
      "loss": 2.0478,
      "step": 7810
    },
    {
      "epoch": 1.9091796875,
      "grad_norm": 4.177576541900635,
      "learning_rate": 1.8184407552083336e-05,
      "loss": 2.0677,
      "step": 7820
    },
    {
      "epoch": 1.91162109375,
      "grad_norm": 4.976147651672363,
      "learning_rate": 1.8143717447916667e-05,
      "loss": 1.8596,
      "step": 7830
    },
    {
      "epoch": 1.9140625,
      "grad_norm": 5.575987815856934,
      "learning_rate": 1.8103027343750002e-05,
      "loss": 2.1784,
      "step": 7840
    },
    {
      "epoch": 1.91650390625,
      "grad_norm": 4.467540740966797,
      "learning_rate": 1.8062337239583336e-05,
      "loss": 1.8796,
      "step": 7850
    },
    {
      "epoch": 1.9189453125,
      "grad_norm": 3.098818063735962,
      "learning_rate": 1.8021647135416667e-05,
      "loss": 2.0613,
      "step": 7860
    },
    {
      "epoch": 1.92138671875,
      "grad_norm": 7.093335151672363,
      "learning_rate": 1.7980957031250002e-05,
      "loss": 2.0592,
      "step": 7870
    },
    {
      "epoch": 1.923828125,
      "grad_norm": 3.9438326358795166,
      "learning_rate": 1.7940266927083336e-05,
      "loss": 2.1051,
      "step": 7880
    },
    {
      "epoch": 1.92626953125,
      "grad_norm": 5.648283958435059,
      "learning_rate": 1.7899576822916667e-05,
      "loss": 1.853,
      "step": 7890
    },
    {
      "epoch": 1.9287109375,
      "grad_norm": 3.8742125034332275,
      "learning_rate": 1.785888671875e-05,
      "loss": 1.8683,
      "step": 7900
    },
    {
      "epoch": 1.93115234375,
      "grad_norm": 5.858368873596191,
      "learning_rate": 1.7818196614583336e-05,
      "loss": 1.8157,
      "step": 7910
    },
    {
      "epoch": 1.93359375,
      "grad_norm": 5.939318656921387,
      "learning_rate": 1.7777506510416667e-05,
      "loss": 1.9992,
      "step": 7920
    },
    {
      "epoch": 1.93603515625,
      "grad_norm": 4.152792453765869,
      "learning_rate": 1.773681640625e-05,
      "loss": 1.9533,
      "step": 7930
    },
    {
      "epoch": 1.9384765625,
      "grad_norm": 6.6409010887146,
      "learning_rate": 1.7696126302083336e-05,
      "loss": 2.0606,
      "step": 7940
    },
    {
      "epoch": 1.94091796875,
      "grad_norm": 4.767549991607666,
      "learning_rate": 1.7655436197916667e-05,
      "loss": 1.9883,
      "step": 7950
    },
    {
      "epoch": 1.943359375,
      "grad_norm": 4.8669867515563965,
      "learning_rate": 1.761474609375e-05,
      "loss": 1.9748,
      "step": 7960
    },
    {
      "epoch": 1.94580078125,
      "grad_norm": 4.907863140106201,
      "learning_rate": 1.7574055989583336e-05,
      "loss": 1.9167,
      "step": 7970
    },
    {
      "epoch": 1.9482421875,
      "grad_norm": 5.387188911437988,
      "learning_rate": 1.7533365885416667e-05,
      "loss": 1.9479,
      "step": 7980
    },
    {
      "epoch": 1.95068359375,
      "grad_norm": 6.187702178955078,
      "learning_rate": 1.749267578125e-05,
      "loss": 2.0438,
      "step": 7990
    },
    {
      "epoch": 1.953125,
      "grad_norm": 5.789539337158203,
      "learning_rate": 1.7451985677083336e-05,
      "loss": 2.0626,
      "step": 8000
    },
    {
      "epoch": 1.95556640625,
      "grad_norm": 4.051547050476074,
      "learning_rate": 1.7411295572916667e-05,
      "loss": 1.9707,
      "step": 8010
    },
    {
      "epoch": 1.9580078125,
      "grad_norm": 6.103731632232666,
      "learning_rate": 1.737060546875e-05,
      "loss": 2.0319,
      "step": 8020
    },
    {
      "epoch": 1.96044921875,
      "grad_norm": 3.861697196960449,
      "learning_rate": 1.7329915364583336e-05,
      "loss": 1.7799,
      "step": 8030
    },
    {
      "epoch": 1.962890625,
      "grad_norm": 5.110465049743652,
      "learning_rate": 1.7289225260416667e-05,
      "loss": 2.057,
      "step": 8040
    },
    {
      "epoch": 1.96533203125,
      "grad_norm": 5.329772472381592,
      "learning_rate": 1.724853515625e-05,
      "loss": 2.0577,
      "step": 8050
    },
    {
      "epoch": 1.9677734375,
      "grad_norm": 4.313917636871338,
      "learning_rate": 1.7207845052083336e-05,
      "loss": 1.918,
      "step": 8060
    },
    {
      "epoch": 1.97021484375,
      "grad_norm": 6.6954522132873535,
      "learning_rate": 1.7167154947916667e-05,
      "loss": 2.03,
      "step": 8070
    },
    {
      "epoch": 1.97265625,
      "grad_norm": 4.122062683105469,
      "learning_rate": 1.712646484375e-05,
      "loss": 1.9356,
      "step": 8080
    },
    {
      "epoch": 1.97509765625,
      "grad_norm": 5.584367275238037,
      "learning_rate": 1.7085774739583335e-05,
      "loss": 1.9297,
      "step": 8090
    },
    {
      "epoch": 1.9775390625,
      "grad_norm": 7.211095333099365,
      "learning_rate": 1.7045084635416666e-05,
      "loss": 2.0169,
      "step": 8100
    },
    {
      "epoch": 1.97998046875,
      "grad_norm": 5.701065540313721,
      "learning_rate": 1.700439453125e-05,
      "loss": 1.9029,
      "step": 8110
    },
    {
      "epoch": 1.982421875,
      "grad_norm": 3.5021862983703613,
      "learning_rate": 1.6963704427083335e-05,
      "loss": 2.1369,
      "step": 8120
    },
    {
      "epoch": 1.98486328125,
      "grad_norm": 5.204845428466797,
      "learning_rate": 1.6923014322916666e-05,
      "loss": 1.8819,
      "step": 8130
    },
    {
      "epoch": 1.9873046875,
      "grad_norm": 5.023906230926514,
      "learning_rate": 1.688232421875e-05,
      "loss": 1.8739,
      "step": 8140
    },
    {
      "epoch": 1.98974609375,
      "grad_norm": 5.295637130737305,
      "learning_rate": 1.6841634114583335e-05,
      "loss": 1.8454,
      "step": 8150
    },
    {
      "epoch": 1.9921875,
      "grad_norm": 4.685945987701416,
      "learning_rate": 1.6800944010416666e-05,
      "loss": 1.9475,
      "step": 8160
    },
    {
      "epoch": 1.99462890625,
      "grad_norm": 6.034104824066162,
      "learning_rate": 1.676025390625e-05,
      "loss": 2.1096,
      "step": 8170
    },
    {
      "epoch": 1.9970703125,
      "grad_norm": 4.586907386779785,
      "learning_rate": 1.6719563802083335e-05,
      "loss": 1.8573,
      "step": 8180
    },
    {
      "epoch": 1.99951171875,
      "grad_norm": 4.797008991241455,
      "learning_rate": 1.6678873697916666e-05,
      "loss": 1.9205,
      "step": 8190
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.7986676692962646,
      "eval_runtime": 5.6105,
      "eval_samples_per_second": 22.814,
      "eval_steps_per_second": 22.814,
      "step": 8192
    }
  ],
  "logging_steps": 10,
  "max_steps": 12288,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 540238166360064.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
